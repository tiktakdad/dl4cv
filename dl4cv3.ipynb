{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Preparation\n",
    "\n",
    "     colab -> 런타임 -> 런타임 유형 변경 -> GPU -> 저장"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "ready.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from copy import deepcopy # Add Deepcopy for args\n",
    "import matplotlib.pyplot as plt\n",
    "import torchsummary\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "partition = {'train': trainset, 'val':valset, 'test':testset}\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# dummy class\n",
    "class Resnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Resnet, self).__init__()\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "class Densenet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Densenet, self).__init__()\n",
    "\n",
    "def print_acc_class(net, partition):\n",
    "    testloader = torch.utils.data.DataLoader(partition['test'],\n",
    "                                         batch_size=args.test_batch_size,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "    net.eval()\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(4):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    for i in range(10):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "\n",
    "def plot_loss_variation(result, **kwargs):\n",
    "    list_data = []\n",
    "    for epoch, train_loss in enumerate(result['train_losses']):\n",
    "        list_data.append({'type': 'train', 'loss': train_loss, 'epoch': epoch})\n",
    "    for epoch, val_loss in enumerate(result['val_losses']):\n",
    "        list_data.append({'type': 'val', 'loss': val_loss, 'epoch': epoch})\n",
    "\n",
    "    df = pd.DataFrame(list_data)\n",
    "    g = sns.FacetGrid(df, hue='type', **kwargs)\n",
    "    g = g.map(plt.plot, 'epoch', 'loss', marker='.')\n",
    "    g.add_legend()\n",
    "    g.fig.suptitle('Train loss vs Val loss')\n",
    "    plt.subplots_adjust(top=0.89)\n",
    "\n",
    "\n",
    "def plot_acc_variation(result, **kwargs):\n",
    "    list_data = []\n",
    "    for epoch, train_acc in enumerate(result['train_accs']):\n",
    "        list_data.append({'type': 'train', 'Acc': train_acc, 'test_acc': result['test_acc'], 'epoch': epoch})\n",
    "    for epoch, val_acc in enumerate(result['val_accs']):\n",
    "        list_data.append({'type': 'val', 'Acc': val_acc, 'test_acc': result['test_acc'], 'epoch': epoch})\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(list_data)\n",
    "    g = sns.FacetGrid(df, hue='type', **kwargs)\n",
    "    g = g.map(plt.plot, 'epoch', 'Acc', marker='.')\n",
    "\n",
    "    def show_acc(x, y, metric, **kwargs):\n",
    "        plt.scatter(x, y, alpha=0.3, s=1)\n",
    "        metric = \"Test Acc: {:1.3f}\".format(list(metric.values)[0])\n",
    "        plt.text(0.05, 0.95, metric, horizontalalignment='left', verticalalignment='center',\n",
    "                 transform=plt.gca().transAxes, bbox=dict(facecolor='yellow', alpha=0.5, boxstyle=\"round,pad=0.1\"))\n",
    "\n",
    "    g = g.map(show_acc, 'epoch', 'Acc', 'test_acc')\n",
    "\n",
    "    g.add_legend()\n",
    "    g.fig.suptitle('Train Accuracy vs Val Accuracy')\n",
    "    plt.subplots_adjust(top=0.89)\n",
    "\n",
    "def train(net, partition, optimizer, criterion, args):\n",
    "    trainloader = torch.utils.data.DataLoader(partition['train'],\n",
    "                                              batch_size=args.train_batch_size,\n",
    "                                              shuffle=True, num_workers=0)\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc = 100 * correct / total\n",
    "    return net, train_loss, train_acc\n",
    "\n",
    "\n",
    "def validate(net, partition, criterion, args):\n",
    "    valloader = torch.utils.data.DataLoader(partition['val'],\n",
    "                                            batch_size=args.test_batch_size,\n",
    "                                            shuffle=False, num_workers=0)\n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            images, labels = data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = net(images)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(valloader)\n",
    "        val_acc = 100 * correct / total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def test(net, partition, args):\n",
    "    testloader = torch.utils.data.DataLoader(partition['test'],\n",
    "                                             batch_size=args.test_batch_size,\n",
    "                                             shuffle=False, num_workers=0)\n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_acc = 100 * correct / total\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "\n",
    "def experiment(partition, args):\n",
    "    if args.model == 'CNN':\n",
    "        net = CNN()\n",
    "    elif args.model == 'Resnet':\n",
    "        net = Resnet()\n",
    "    elif args.model == 'SE_Resnet':\n",
    "        net = SE_Resnet()\n",
    "    elif args.model == 'Densenet':\n",
    "        net = Densenet()\n",
    "    else:\n",
    "        raise ValueError('In-valid model choice')\n",
    "    net.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
    "        ts = time.time()\n",
    "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
    "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
    "        te = time.time()\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(\n",
    "            'Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch,\n",
    "                                                                                                                  train_acc,\n",
    "                                                                                                                  val_acc,\n",
    "                                                                                                                  train_loss,\n",
    "                                                                                                                  val_loss,\n",
    "                                                                                                                  te - ts))\n",
    "\n",
    "    test_acc = test(net, partition, args)\n",
    "    print_acc_class(net, partition)\n",
    "\n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['train_accs'] = train_accs\n",
    "    result['val_accs'] = val_accs\n",
    "    result['train_acc'] = train_acc\n",
    "    result['val_acc'] = val_acc\n",
    "    result['test_acc'] = test_acc\n",
    "    return vars(args), result\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "print('ready.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Going deeper: Case studies\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide74.PNG?raw=true)\n",
    "\n",
    "    ImageNet 데이터 베이스를 이용해서 1000개의 클래스에 대한 top-5 error 그래프입니다.\n",
    "    cifar에 비해서 이미지도 크고(227x227x3) 분류해야될 클래스도 많기 때문에 어렵겠죠? 데이터셋도 더 크구요.\n",
    "    이 network들이 어떠한 테크닉을 사용하였는지, case study를 통해 같이 살펴보겠습니다.\n",
    "***\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.2 AlexNet\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide71.PNG?raw=true)\n",
    "\n",
    "    CNN을 사용해서 Imagenet Challange에서 우승한 최초의 딥러닝 네트워크인 alexnet입니다.\n",
    "    논문에도 아키텍쳐 다이어그램이 위에 짤린것으로 나타내는 데, 정확히는 모르겠지만 의도적으로 한것같다.\n",
    "    이 아키텍처 다이그램이 보통적으로 보이지 않는 이유는 2gpu를 사용해서 큰 모델을 학습시키기 위해서,\n",
    "    two-stream network입니다.\n",
    "    학습하는데, 6일이 걸렸음, 지금은 몇분만에 학습가능\n",
    "    alexnet의 특징은 처음으로 CNN을 제대로 사용할 수 있다고 말씀드릴 수 있겠는데요,\n",
    "    첫번째는 vanishing Gradient 문제를  해결하기위해 ReLU를 사용했으며,\n",
    "    overfitting을 피하기 위해 dropout, weight decay를 사용했습니다.\n",
    "    weight decay는 특정한 weight값이 너무 크면 큰 만큼 값을 줄여줘서(부식시킨다) regularization해주는 겁니다.\n",
    "    (신경망이 범용성을 갖도록 처리하는 거니까 regularization은 일반화 정도라고 생각하시면됩니다 normalization, 정규화와는 다른의미입니다.)\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.2 AlexNet\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide72.PNG?raw=true)\n",
    "\n",
    "    alexnet을 단순화 시켜보면 이렇습니다. input을 conv, non-linaer, pooing 반복반복,\n",
    "    마지막에 fcfcfc, lenet, cnn의 기본구조와는 거의 차이가없죠? 그래서 이 친구 같은 경우는 아직 advanced cnn은 아니다.\n",
    "    처음으로 convolution layer를 dnn에 잘 적용했기때문에 다뤄보았구요,\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.3 VGGNet\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/Lec6-A.pdf_page_17.png?raw=true)\n",
    "\n",
    "    이제부터 중요해지는 그 다음것을 봅시다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.3 VGGNet\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide75.PNG?raw=true)\n",
    "\n",
    "    VGGNet은 2014년도에 2위를 한 oxford대학의 네트워크입니다. 1위는 GoogLeNet이 했죠.\n",
    "    이 네트워크를 다루는 이유는 VGGNet의 네트워크 디자인 룰이 간단하고 효과적이기 때문에 아직까지도 많은 영향을 미치고 있기 때문입니다.\n",
    "    네트워크를 간단히 살펴보자면, conv,conv,pooling -> .... -> conv,conv,conv,conv,poopling,fcfcfc 구조네요.\n",
    "    conv를 연속적으로 한다는 것 빼고는 딱히 alexnet과 큰 차이는 없지만, 우리는 Design rule을 볼 필요가 있습니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.3 VGGNet\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/498_FA2019_lecture08.pdf_page_041.png?raw=true)\n",
    "\n",
    "    첫번째 디자인 룰은 same convolution을 합니다. 위에서 한번 말씀드렸듯이 padding 1을 추가해서 feature map이 작아지는 것을\n",
    "    방지하여 좀 더 깊은 네트워크를 만들 수 있게 합니다. 그리고 conv-pooling을 하는 이전과는 다르게 conv-conv-pooling을 하는\n",
    "    구조를 보실 수 있으실텐데요, 우리는 가로세로 5칸의 window를 featuremap에 담고싶다고 하면,\n",
    "    (이 윈도우를 receptive field라 위에서 말씀드렸죠?) 5x5 conv를 한번 하는것보다,\n",
    "    3x3을 두번 하는 것이 효과적이다라는 것을 찾아내었기 때문인데요, 아래보시면\n",
    "    파라미터 수와 실수연산은 더 적은데 같은효과 오히려 non-linearity를 두 번 적용한 더 좋은 결과가 나온다는 것을 보실 수 있습니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.3 VGGNet\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide76.PNG?raw=true)\n",
    "    두번째, 첫번째 레이어의 5x5 receptive filed를 보는 것과 같은 효과라는 것을 도식화 한 그림입니다.\n",
    "    왜냐하면 첫번째 conv-layer를 통과한 fateure-map의 한 칸이 input 3x3 정보를 담고 있기 때문이죠.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.3 VGGNet\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/498_FA2019_lecture08.pdf_page_044.png?raw=true)\n",
    "\n",
    "    두번째 룰은 plooing을 할때, feature map을 반으로 줄이고 채널 수를 두배로 키워줍니다.\n",
    "    이 룰은 아주 많은 cnn 아키텍쳐들이 따르고 있는 룰인데요,\n",
    "    각 conv스테이지마다 메모리는 줄이고 실수 연산cost는 유지하고 싶었기 때문입니다.\n",
    "    (Memory=H*W*C , Params=c_in*c_out*k_h*k_w, Params=(k_h*k_w*k_z)*filters_num, FLOPs=(c_out*h*w)*(c_in*k_h*k_w)\n",
    "***\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.3 VGGNet\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide77.PNG?raw=true)\n",
    "\n",
    "    이 그래프는 VGG의 imagenet top-5 error인데요, 16레이어까지는 깊게 깧을 수록 에러가 낮아지는 것을 볼 수 있는데,\n",
    "    더 깊게 쌓으면 에러가 차라리 높아지는 것을 볼 수 있습니다.\n",
    "    깊어질수록 많아지는 연산량과 weight paramter들을 optimize를 하는 것이 어려움을 어떠한 네트워크 디자인을\n",
    "    통해 극복했는지 살펴보도록 합시다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.5 GooLeNet: Inception module\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide92.PNG?raw=true)\n",
    "\n",
    "    14년도 Imagenet challange(ILSVRC) 1위를 한 GooLeNet입니다. 2위는 VGG구요.\n",
    "    GooLeNet에서 중간에 L자가 대문자인 이유는 얀르쿤교수팀이 개발한 Lenet을 shoutout하는 의미로 하였구요.\n",
    "    네트워크 아키텍쳐가 조금 복잡해보이는데요, 가장 눈에 띄는점은 뭐 이렇게 묶음이 있구요, 중간중간 곁가지 처럼 나온게 있네요.\n",
    "    이 묶음이랑 곁가지를 제외하고 본다면 일반적인 CNN구조와 유사하게 매우 단순한 구조로 가지고 있습니다.\n",
    "\n",
    "    우선 옆으로 나온 곁가지에 대해 설명드리자면, 끝에서 나온 loss는 초반부 layer에 영향이 약해지는 vanishing gardient문제를\n",
    "    조금이라도 보완하고자, 중간중간 loss도 뽑아서 역전파를 시키자는 trick입니다.\n",
    "    15년도에 batch normalizaion이라는 학습 방법이 나온 뒤는 더이상 사용하지 않습니다.\n",
    "\n",
    "    그럼 이 묶음은 뭘까요? 이 묶음에 대해서 자세히 알아보도록 합시다.\n",
    "\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.5 GooLeNet: Inception module\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide93.PNG?raw=true)\n",
    "\n",
    "    이 묶음의 이름은 inception module이라고 합니다.\n",
    "    영화 인셉션에서 \"우리는 좀더 깊이 들어가야해.\"라는 대사가 나옵니다. 영화에선 꿈의 심연을 뜻하는거겠죠?\n",
    "    딥러닝에서도 좀 더 깊이 네트워크를 쌓고자하는 바램에서 inception이란 이름을 따왔습니다.\n",
    "\n",
    "    inception module을 크게 확하면 왼쪽과 같은데, parallel한 구조를 가지고 있습니다.\n",
    "    1x1 conv를 빼고보자면, 그냥 input도 넘겨주고,\n",
    "    3x3 conv한 것도 넘겨주고, 5x5 conv한것도 넘겨주고, 3x3 pooling 한것도 넘겨주네요,\n",
    "    한가지 input에 대해서 동시에 다양한 방면으로 본다는 의미에서 좋아 보입니다. 그리고 마지막에 합치는 것을 볼 수 있네요\n",
    "    여기서 합치는 것을 concatenate, stack 이라고 부르는데 값을 합치는 것이 아닌, 단순히 채널들을 순서대로 쌓는다는 의미입니다.\n",
    "\n",
    "    1x1 conv는 이렇게 합치다보면 너무 channel이 두꺼워지니까 1x1 conv를 통해 줄이는 연산입니다.\n",
    "    넓이와 높이를 줄이는데 pooling이 있듯이 channel을 줄이는데는 1x1 conv를 사용할 수 있으며\n",
    "    이 레이어를 병목처럼 줄인다고하여 bottleneck layer라고 합니다.\n",
    "\n",
    "    좀 더 쉽게 설명하자면 식빵이 들어오면, 이 식빵을 4개로 잘라서 다양한 4명의 사람이 살펴보고 특이한 것을 마킹을하고\n",
    "    다시 식빵을 합치는 겁니다. 근데 식빵이 이 과정을 할때마다 두꺼워져서 양옆에서 손으로 한번씩 눌러줘서 원래크기 비슷하게 맞춰줍니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.5 GooLeNet: batch normalization\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/covariate_shift.jpg?raw=true)\n",
    "\n",
    "    그리고 15년도에 batch norm이 나온 후 부터는 곁가지 트릭은 쓰지 않는다고 말씀드렸습니다.\n",
    "    batch normalization, batch norm/bn이라고 부르는 이 방법은.\n",
    "    학습시에 현재 layer의 입력은 모든 이전 layer의 파라미터의 변화에 영향을 받게 되며,\n",
    "    망이 깊어짐에 따라 이전 layer에서의 작은 파라미터 변화가 증폭되어 뒷단에 큰 영향을 끼치게 되는 문제가 발생할 수 있습니다.\n",
    "    마치 나비효과처럼요. 이것을 “Covariate Shift”라고 합니다.\n",
    "    그래서 우리는 중간중간 이를 바로잡아줄 방법이 필요한데 이게 바로 batch norm입니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.5 GooLeNet: batch normalization\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide80.PNG?raw=true)\n",
    "\n",
    "    학습은 mini-batch단위로 진행하기 때문에 mini-batch의 평균과 분산을 구해서, 데이터를 -1~1사이로 normalization을 시켜줍니다.\n",
    "    우리는 normalization에 scale값과 shift를 추가하여 이 변수도 같이 학습을 통해 정하게 하여,\n",
    "    전체 dataset을 normalization을 해주는 factor를 찾게하여 단순하게 정규화만을 할 때 보다 훨씬 강력하게 만듭니다.\n",
    "\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.5 GooLeNet: batch normalization\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide81.PNG?raw=true)\n",
    "\n",
    "    구글의 inception-v2부터는 bn을 적용하는데, 보시는 것처럼 같은 네트워크에서 bn이 없을때와 bn이 있을때\n",
    "    얼마나 잘 network 학습하는지, 정확도가 나오는지 증명하고 있습니다.\n",
    "    학습 진도율을 키운 BN-x30의 경우는 기존 Inception 보다 6배 빠르게 학습되며 더 높은 정확도에 도달할 수 있었습니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.5 GooLeNet: batch normalization\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/lecture_1_ranjay.pdf_page_10.png?raw=true)\n",
    "\n",
    "    batch norm과 다양한 딥러닝 학습 테크닉의 발견으로 우리는 더 network를 깊게 쌓을 수 있었습니다.\n",
    "    15년도 이후 논문들의 layer수를 보면 갑자기 100개 넘는 것을 볼 수 있습니다.\n",
    "    정확도도 눈에 띄게 향상됬구요.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.6 ResNet\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/498_FA2019_lecture08.pdf_page_060.png?raw=true)\n",
    "\n",
    "    Batch norm을 이용해서 10개 이상의 레이어를 학습시킬 수 있게됬습니다.\n",
    "    그런데 오른쪽에 보이는 것처럼 Test dataset에 대한 에러는 56개를 쌓은 에러가 더 높네요.\n",
    "    아마도 레이어가 깊다보니 학습데이터 셋을 외워버리는 overfitting일 일어난 것 같습니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.6 ResNet\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/498_FA2019_lecture08.pdf_page_061.png?raw=true)\n",
    "\n",
    "    그런데 training error도 20개를 쌓은 비교적 shallow한 network보다 에러가 높습니다.\n",
    "    사실은 overfitting이 아닌 학습이 잘 되지않은 underfitting이 일어난 겁니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.6 ResNet\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/498_FA2019_lecture08.pdf_page_063.png?raw=true)\n",
    "\n",
    "    깊은 모델은 얕은 모델보다 더 성능이 잘 나와야합니다.\n",
    "    예를들어 20개의 layer를 가진 모델 뒤에 아무런일을 하지 않는 36개 레이어를 붙인다고해도 성능이 떨어질 이유는 없기 때문이죠.\n",
    "    여기서 깊은 네트워크는 가장 낮은 loss를 찾는 weight를 optimize를 찾기힘들것이라는 추측을 합니다.\n",
    "    그래서 우리는 앞에 20개의 레이어를 유지하면서 뒤에 36개는 조금만 추가적으로 일해도 잘 나올것이다라는 생각처럼,\n",
    "    앞에 레이어의 정보를 유지하면서 추가 정보를 붙이는 identity function을 설계하게 됩니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.6 ResNet\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide82.PNG?raw=true)\n",
    "\n",
    "    이 그림은 resnet의 residual block이라고 부르는 module입니다.\n",
    "    inception module처럼 resnet도 이 residual block을 반복하면서 네트워크를 만듭니다.\n",
    "    위에서 언급했었것 처럼 indentity를 유지하기위해 input그대로와 convolution결과를 더합니다.\n",
    "    여기서 더하는 것은 inception module처럼 concatenate를 하는 것은 아니고 두 레이어를 더하는 겁니다.\n",
    "    역전파를 할 때 더하기 연산은 값을 그대로 복사하기 때문에 이전 feature 공유의 indentity를 유지할 수 있습니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.6 ResNet\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide84.PNG?raw=true)\n",
    "\n",
    "    이렇게 residual block을 쌓아서 18개부터 152개 까지의 다양한 resnet 아키텍쳐를 만듭니다.\n",
    "    보시면 inception구조처럼 1x1 conv를 통해서 채널 수를 줄여주고 있는 것을 보실 수 있습니다.\n",
    "    이 방법을 통해서 깊게 쌓으면서도 연산은 VGG-16(13.6)보다 적게 사용하고 있습니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: Case studies\n",
    "### 3.4.6 ResNet\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/498_FA2019_lecture08.pdf_page_079.png?raw=true)\n",
    "\n",
    "    이 모델 아키텍쳐는 기대한대로 깊게 쌓을수록 네트워크 성능이 좋아진다는 것을 증명하였고,\n",
    "    resnet을 backbone으로 한 5개의 challage 분야에서 1위를 달성했습니다.\n",
    "    매우 심플한 network design에 좋은 성능을 내어 아직까지도 많이 사랑받는 network입니다.\n",
    "\n",
    "    그럼 실습을 통해 Resnet구현을 해보겠습니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(act='relu', epoch=300, exp_name='exp1_lr', l2=1e-05, lr=0.001, model='Resnet', optim='SGD', test_batch_size=64, train_batch_size=256)\n",
      "Epoch 0, Acc(train/val): 26.95/22.55, Loss(train/val) 2.04/5.41. Took 74.45 sec\n",
      "Epoch 1, Acc(train/val): 42.13/42.76, Loss(train/val) 1.61/1.63. Took 73.94 sec\n",
      "Epoch 2, Acc(train/val): 47.76/47.15, Loss(train/val) 1.47/1.69. Took 73.61 sec\n",
      "Epoch 3, Acc(train/val): 53.27/49.14, Loss(train/val) 1.33/1.47. Took 73.49 sec\n",
      "Epoch 4, Acc(train/val): 58.27/56.40, Loss(train/val) 1.18/1.25. Took 73.79 sec\n",
      "Epoch 5, Acc(train/val): 63.49/58.86, Loss(train/val) 1.04/1.15. Took 73.54 sec\n",
      "Epoch 6, Acc(train/val): 68.87/59.50, Loss(train/val) 0.89/1.21. Took 73.77 sec\n",
      "Epoch 7, Acc(train/val): 71.77/59.72, Loss(train/val) 0.82/1.18. Took 73.71 sec\n",
      "Epoch 8, Acc(train/val): 75.89/60.43, Loss(train/val) 0.69/1.34. Took 73.67 sec\n",
      "Epoch 9, Acc(train/val): 78.60/61.18, Loss(train/val) 0.62/1.36. Took 73.58 sec\n",
      "Epoch 10, Acc(train/val): 81.47/64.72, Loss(train/val) 0.53/1.25. Took 73.70 sec\n",
      "Epoch 11, Acc(train/val): 86.50/65.74, Loss(train/val) 0.41/1.39. Took 73.84 sec\n",
      "Epoch 12, Acc(train/val): 86.91/63.87, Loss(train/val) 0.40/1.47. Took 73.50 sec\n",
      "Epoch 13, Acc(train/val): 88.08/65.21, Loss(train/val) 0.35/1.44. Took 73.67 sec\n",
      "Epoch 14, Acc(train/val): 91.15/68.95, Loss(train/val) 0.26/1.40. Took 73.44 sec\n",
      "Epoch 15, Acc(train/val): 93.31/68.66, Loss(train/val) 0.20/1.43. Took 73.51 sec\n",
      "Epoch 16, Acc(train/val): 94.81/69.18, Loss(train/val) 0.15/1.48. Took 73.38 sec\n",
      "Epoch 17, Acc(train/val): 95.53/70.03, Loss(train/val) 0.13/1.57. Took 73.50 sec\n",
      "Epoch 18, Acc(train/val): 96.22/69.21, Loss(train/val) 0.11/1.60. Took 73.70 sec\n",
      "Epoch 19, Acc(train/val): 96.22/70.18, Loss(train/val) 0.11/1.60. Took 73.68 sec\n",
      "Epoch 20, Acc(train/val): 96.66/69.21, Loss(train/val) 0.10/1.69. Took 73.57 sec\n",
      "Epoch 21, Acc(train/val): 96.47/67.42, Loss(train/val) 0.10/1.86. Took 73.98 sec\n",
      "Epoch 22, Acc(train/val): 95.92/68.81, Loss(train/val) 0.12/1.70. Took 73.85 sec\n",
      "Epoch 23, Acc(train/val): 96.78/70.35, Loss(train/val) 0.09/1.59. Took 73.73 sec\n",
      "Epoch 24, Acc(train/val): 98.09/71.50, Loss(train/val) 0.06/1.59. Took 73.52 sec\n",
      "Epoch 25, Acc(train/val): 98.57/71.91, Loss(train/val) 0.04/1.71. Took 73.54 sec\n",
      "Epoch 26, Acc(train/val): 98.89/72.55, Loss(train/val) 0.03/1.69. Took 73.90 sec\n",
      "Epoch 27, Acc(train/val): 99.00/71.16, Loss(train/val) 0.03/1.83. Took 73.55 sec\n",
      "Epoch 28, Acc(train/val): 99.13/72.86, Loss(train/val) 0.03/1.75. Took 73.36 sec\n",
      "Epoch 29, Acc(train/val): 99.20/71.92, Loss(train/val) 0.02/1.80. Took 73.33 sec\n",
      "Epoch 30, Acc(train/val): 99.33/72.22, Loss(train/val) 0.02/1.80. Took 73.74 sec\n",
      "Epoch 31, Acc(train/val): 99.50/73.06, Loss(train/val) 0.01/1.75. Took 73.79 sec\n",
      "Epoch 32, Acc(train/val): 99.58/73.30, Loss(train/val) 0.01/1.73. Took 73.98 sec\n",
      "Epoch 33, Acc(train/val): 99.53/72.92, Loss(train/val) 0.02/1.81. Took 73.79 sec\n",
      "Epoch 34, Acc(train/val): 99.55/72.65, Loss(train/val) 0.01/1.82. Took 73.57 sec\n",
      "Epoch 35, Acc(train/val): 99.32/72.53, Loss(train/val) 0.02/1.87. Took 73.74 sec\n",
      "Epoch 36, Acc(train/val): 99.17/72.55, Loss(train/val) 0.02/1.75. Took 73.50 sec\n",
      "Epoch 37, Acc(train/val): 99.44/73.38, Loss(train/val) 0.02/1.77. Took 73.58 sec\n",
      "Epoch 38, Acc(train/val): 99.60/72.85, Loss(train/val) 0.01/1.79. Took 73.74 sec\n",
      "Epoch 39, Acc(train/val): 99.60/72.68, Loss(train/val) 0.01/1.86. Took 73.84 sec\n",
      "Epoch 40, Acc(train/val): 99.49/72.90, Loss(train/val) 0.01/1.84. Took 73.53 sec\n",
      "Epoch 41, Acc(train/val): 99.52/73.16, Loss(train/val) 0.02/1.83. Took 73.87 sec\n",
      "Epoch 42, Acc(train/val): 99.50/73.16, Loss(train/val) 0.02/1.87. Took 73.49 sec\n",
      "Epoch 43, Acc(train/val): 99.62/74.02, Loss(train/val) 0.01/1.77. Took 73.50 sec\n",
      "Epoch 44, Acc(train/val): 99.72/74.04, Loss(train/val) 0.01/1.78. Took 73.60 sec\n",
      "Epoch 45, Acc(train/val): 99.78/73.86, Loss(train/val) 0.01/1.88. Took 73.48 sec\n",
      "Epoch 46, Acc(train/val): 99.80/73.73, Loss(train/val) 0.01/1.85. Took 73.67 sec\n",
      "Epoch 47, Acc(train/val): 99.82/74.17, Loss(train/val) 0.01/1.88. Took 73.54 sec\n",
      "Epoch 48, Acc(train/val): 99.85/73.73, Loss(train/val) 0.00/1.92. Took 73.67 sec\n",
      "Epoch 49, Acc(train/val): 99.69/73.71, Loss(train/val) 0.01/1.90. Took 73.48 sec\n",
      "Epoch 50, Acc(train/val): 99.72/72.95, Loss(train/val) 0.01/2.07. Took 73.44 sec\n",
      "Epoch 51, Acc(train/val): 99.57/73.18, Loss(train/val) 0.01/2.03. Took 73.45 sec\n",
      "Epoch 52, Acc(train/val): 99.31/72.93, Loss(train/val) 0.02/2.06. Took 73.66 sec\n",
      "Epoch 53, Acc(train/val): 99.03/71.93, Loss(train/val) 0.03/2.04. Took 73.56 sec\n",
      "Epoch 54, Acc(train/val): 98.67/72.05, Loss(train/val) 0.04/1.92. Took 73.33 sec\n",
      "Epoch 55, Acc(train/val): 98.67/71.11, Loss(train/val) 0.04/1.97. Took 73.98 sec\n",
      "Epoch 56, Acc(train/val): 98.58/72.08, Loss(train/val) 0.04/1.84. Took 73.56 sec\n",
      "Epoch 57, Acc(train/val): 99.04/72.11, Loss(train/val) 0.03/1.87. Took 73.82 sec\n",
      "Epoch 58, Acc(train/val): 99.31/73.26, Loss(train/val) 0.02/1.81. Took 74.00 sec\n",
      "Epoch 59, Acc(train/val): 99.36/72.94, Loss(train/val) 0.02/1.98. Took 73.67 sec\n",
      "Epoch 60, Acc(train/val): 99.29/72.77, Loss(train/val) 0.02/1.97. Took 73.72 sec\n",
      "Epoch 61, Acc(train/val): 99.41/73.19, Loss(train/val) 0.02/1.91. Took 73.61 sec\n",
      "Epoch 62, Acc(train/val): 99.52/72.55, Loss(train/val) 0.01/1.92. Took 73.80 sec\n",
      "Epoch 63, Acc(train/val): 99.70/74.44, Loss(train/val) 0.01/1.77. Took 73.54 sec\n",
      "Epoch 64, Acc(train/val): 99.84/74.77, Loss(train/val) 0.00/1.86. Took 73.62 sec\n",
      "Epoch 65, Acc(train/val): 99.89/75.08, Loss(train/val) 0.00/1.76. Took 73.58 sec\n",
      "Epoch 66, Acc(train/val): 99.96/75.69, Loss(train/val) 0.00/1.73. Took 73.51 sec\n",
      "Epoch 67, Acc(train/val): 100.00/76.04, Loss(train/val) 0.00/1.72. Took 74.01 sec\n",
      "Epoch 68, Acc(train/val): 99.99/75.76, Loss(train/val) 0.00/1.76. Took 73.73 sec\n",
      "Epoch 69, Acc(train/val): 99.99/76.03, Loss(train/val) 0.00/1.70. Took 73.88 sec\n",
      "Epoch 70, Acc(train/val): 99.99/75.93, Loss(train/val) 0.00/1.73. Took 73.87 sec\n",
      "Epoch 71, Acc(train/val): 100.00/76.06, Loss(train/val) 0.00/1.71. Took 73.42 sec\n",
      "Epoch 72, Acc(train/val): 100.00/76.07, Loss(train/val) 0.00/1.70. Took 73.60 sec\n",
      "Epoch 73, Acc(train/val): 100.00/76.25, Loss(train/val) 0.00/1.70. Took 73.87 sec\n",
      "Epoch 74, Acc(train/val): 100.00/76.04, Loss(train/val) 0.00/1.68. Took 73.34 sec\n",
      "Epoch 75, Acc(train/val): 99.99/76.20, Loss(train/val) 0.00/1.69. Took 73.60 sec\n",
      "Epoch 76, Acc(train/val): 99.99/76.30, Loss(train/val) 0.00/1.68. Took 73.76 sec\n",
      "Epoch 77, Acc(train/val): 100.00/76.18, Loss(train/val) 0.00/1.70. Took 73.62 sec\n",
      "Epoch 78, Acc(train/val): 100.00/76.20, Loss(train/val) 0.00/1.68. Took 73.69 sec\n",
      "Epoch 79, Acc(train/val): 100.00/76.26, Loss(train/val) 0.00/1.69. Took 73.76 sec\n",
      "Epoch 80, Acc(train/val): 100.00/76.15, Loss(train/val) 0.00/1.69. Took 73.65 sec\n",
      "Epoch 81, Acc(train/val): 100.00/76.23, Loss(train/val) 0.00/1.68. Took 73.64 sec\n",
      "Epoch 82, Acc(train/val): 100.00/76.32, Loss(train/val) 0.00/1.69. Took 73.66 sec\n",
      "Epoch 83, Acc(train/val): 100.00/76.43, Loss(train/val) 0.00/1.69. Took 73.62 sec\n",
      "Epoch 84, Acc(train/val): 100.00/76.40, Loss(train/val) 0.00/1.68. Took 73.56 sec\n",
      "Epoch 85, Acc(train/val): 100.00/76.18, Loss(train/val) 0.00/1.70. Took 73.49 sec\n",
      "Epoch 86, Acc(train/val): 100.00/76.09, Loss(train/val) 0.00/1.70. Took 73.55 sec\n",
      "Epoch 87, Acc(train/val): 100.00/76.32, Loss(train/val) 0.00/1.70. Took 73.65 sec\n",
      "Epoch 88, Acc(train/val): 100.00/76.28, Loss(train/val) 0.00/1.70. Took 73.52 sec\n",
      "Epoch 89, Acc(train/val): 100.00/76.26, Loss(train/val) 0.00/1.69. Took 73.55 sec\n",
      "Epoch 90, Acc(train/val): 100.00/76.30, Loss(train/val) 0.00/1.69. Took 73.72 sec\n",
      "Epoch 91, Acc(train/val): 100.00/76.45, Loss(train/val) 0.00/1.69. Took 73.45 sec\n",
      "Epoch 92, Acc(train/val): 100.00/76.30, Loss(train/val) 0.00/1.71. Took 74.17 sec\n",
      "Epoch 93, Acc(train/val): 100.00/76.45, Loss(train/val) 0.00/1.70. Took 73.88 sec\n",
      "Epoch 94, Acc(train/val): 100.00/76.44, Loss(train/val) 0.00/1.69. Took 73.69 sec\n",
      "Epoch 95, Acc(train/val): 100.00/76.25, Loss(train/val) 0.00/1.70. Took 73.73 sec\n",
      "Epoch 96, Acc(train/val): 100.00/76.37, Loss(train/val) 0.00/1.70. Took 73.66 sec\n",
      "Epoch 97, Acc(train/val): 100.00/76.35, Loss(train/val) 0.00/1.71. Took 73.92 sec\n",
      "Epoch 98, Acc(train/val): 100.00/76.51, Loss(train/val) 0.00/1.70. Took 73.37 sec\n",
      "Epoch 99, Acc(train/val): 100.00/76.52, Loss(train/val) 0.00/1.70. Took 73.53 sec\n",
      "Epoch 100, Acc(train/val): 100.00/76.47, Loss(train/val) 0.00/1.70. Took 73.81 sec\n",
      "Epoch 101, Acc(train/val): 100.00/76.32, Loss(train/val) 0.00/1.69. Took 73.56 sec\n",
      "Epoch 102, Acc(train/val): 100.00/76.30, Loss(train/val) 0.00/1.70. Took 73.58 sec\n",
      "Epoch 103, Acc(train/val): 100.00/76.53, Loss(train/val) 0.00/1.70. Took 73.70 sec\n",
      "Epoch 104, Acc(train/val): 100.00/76.56, Loss(train/val) 0.00/1.69. Took 73.53 sec\n",
      "Epoch 105, Acc(train/val): 100.00/76.40, Loss(train/val) 0.00/1.69. Took 73.61 sec\n",
      "Epoch 106, Acc(train/val): 100.00/76.38, Loss(train/val) 0.00/1.70. Took 73.47 sec\n",
      "Epoch 107, Acc(train/val): 100.00/76.50, Loss(train/val) 0.00/1.69. Took 73.61 sec\n",
      "Epoch 108, Acc(train/val): 100.00/76.54, Loss(train/val) 0.00/1.70. Took 73.78 sec\n",
      "Epoch 109, Acc(train/val): 100.00/76.39, Loss(train/val) 0.00/1.71. Took 73.82 sec\n",
      "Epoch 110, Acc(train/val): 100.00/76.29, Loss(train/val) 0.00/1.70. Took 73.78 sec\n",
      "Epoch 111, Acc(train/val): 100.00/76.38, Loss(train/val) 0.00/1.70. Took 73.58 sec\n",
      "Epoch 112, Acc(train/val): 100.00/76.50, Loss(train/val) 0.00/1.70. Took 73.57 sec\n",
      "Epoch 113, Acc(train/val): 100.00/76.26, Loss(train/val) 0.00/1.71. Took 73.55 sec\n",
      "Epoch 114, Acc(train/val): 100.00/76.43, Loss(train/val) 0.00/1.71. Took 73.80 sec\n",
      "Epoch 115, Acc(train/val): 100.00/76.58, Loss(train/val) 0.00/1.70. Took 73.76 sec\n",
      "Epoch 116, Acc(train/val): 100.00/76.36, Loss(train/val) 0.00/1.70. Took 73.76 sec\n",
      "Epoch 117, Acc(train/val): 100.00/76.35, Loss(train/val) 0.00/1.71. Took 73.40 sec\n",
      "Epoch 118, Acc(train/val): 100.00/76.42, Loss(train/val) 0.00/1.71. Took 73.61 sec\n",
      "Epoch 119, Acc(train/val): 100.00/76.58, Loss(train/val) 0.00/1.71. Took 73.77 sec\n",
      "Epoch 120, Acc(train/val): 100.00/76.36, Loss(train/val) 0.00/1.70. Took 73.34 sec\n",
      "Epoch 121, Acc(train/val): 100.00/76.51, Loss(train/val) 0.00/1.71. Took 73.59 sec\n",
      "Epoch 122, Acc(train/val): 100.00/76.34, Loss(train/val) 0.00/1.71. Took 73.60 sec\n",
      "Epoch 123, Acc(train/val): 100.00/76.46, Loss(train/val) 0.00/1.71. Took 73.36 sec\n",
      "Epoch 124, Acc(train/val): 100.00/76.54, Loss(train/val) 0.00/1.70. Took 73.87 sec\n",
      "Epoch 125, Acc(train/val): 100.00/76.29, Loss(train/val) 0.00/1.71. Took 73.78 sec\n",
      "Epoch 126, Acc(train/val): 100.00/76.31, Loss(train/val) 0.00/1.71. Took 73.30 sec\n",
      "Epoch 127, Acc(train/val): 100.00/76.42, Loss(train/val) 0.00/1.72. Took 73.48 sec\n",
      "Epoch 128, Acc(train/val): 100.00/76.52, Loss(train/val) 0.00/1.71. Took 73.47 sec\n",
      "Epoch 129, Acc(train/val): 100.00/76.34, Loss(train/val) 0.00/1.71. Took 73.50 sec\n",
      "Epoch 130, Acc(train/val): 100.00/76.37, Loss(train/val) 0.00/1.71. Took 73.61 sec\n",
      "Epoch 131, Acc(train/val): 100.00/76.36, Loss(train/val) 0.00/1.72. Took 73.91 sec\n",
      "Epoch 132, Acc(train/val): 100.00/76.41, Loss(train/val) 0.00/1.71. Took 73.59 sec\n",
      "Epoch 133, Acc(train/val): 100.00/76.48, Loss(train/val) 0.00/1.71. Took 73.55 sec\n",
      "Epoch 134, Acc(train/val): 100.00/76.47, Loss(train/val) 0.00/1.70. Took 73.81 sec\n",
      "Epoch 135, Acc(train/val): 100.00/76.48, Loss(train/val) 0.00/1.72. Took 73.56 sec\n",
      "Epoch 136, Acc(train/val): 100.00/76.45, Loss(train/val) 0.00/1.71. Took 73.65 sec\n",
      "Epoch 137, Acc(train/val): 100.00/76.42, Loss(train/val) 0.00/1.71. Took 73.67 sec\n",
      "Epoch 138, Acc(train/val): 100.00/76.49, Loss(train/val) 0.00/1.71. Took 73.67 sec\n",
      "Epoch 139, Acc(train/val): 100.00/76.43, Loss(train/val) 0.00/1.72. Took 73.49 sec\n",
      "Epoch 140, Acc(train/val): 100.00/76.57, Loss(train/val) 0.00/1.71. Took 73.80 sec\n",
      "Epoch 141, Acc(train/val): 100.00/76.40, Loss(train/val) 0.00/1.72. Took 73.45 sec\n",
      "Epoch 142, Acc(train/val): 100.00/76.40, Loss(train/val) 0.00/1.71. Took 73.45 sec\n",
      "Epoch 143, Acc(train/val): 100.00/76.44, Loss(train/val) 0.00/1.71. Took 73.35 sec\n",
      "Epoch 144, Acc(train/val): 100.00/76.41, Loss(train/val) 0.00/1.72. Took 73.42 sec\n",
      "Epoch 145, Acc(train/val): 100.00/76.49, Loss(train/val) 0.00/1.70. Took 73.37 sec\n",
      "Epoch 146, Acc(train/val): 100.00/76.35, Loss(train/val) 0.00/1.72. Took 73.75 sec\n",
      "Epoch 147, Acc(train/val): 100.00/76.42, Loss(train/val) 0.00/1.72. Took 73.84 sec\n",
      "Epoch 148, Acc(train/val): 100.00/76.35, Loss(train/val) 0.00/1.71. Took 73.95 sec\n",
      "Epoch 149, Acc(train/val): 100.00/76.29, Loss(train/val) 0.00/1.71. Took 73.57 sec\n",
      "Epoch 150, Acc(train/val): 100.00/76.34, Loss(train/val) 0.00/1.71. Took 73.40 sec\n",
      "Epoch 151, Acc(train/val): 100.00/76.34, Loss(train/val) 0.00/1.72. Took 73.71 sec\n",
      "Epoch 152, Acc(train/val): 100.00/76.58, Loss(train/val) 0.00/1.70. Took 73.69 sec\n",
      "Epoch 153, Acc(train/val): 100.00/76.57, Loss(train/val) 0.00/1.72. Took 73.51 sec\n",
      "Epoch 154, Acc(train/val): 100.00/76.50, Loss(train/val) 0.00/1.72. Took 73.41 sec\n",
      "Epoch 155, Acc(train/val): 100.00/76.33, Loss(train/val) 0.00/1.72. Took 73.64 sec\n",
      "Epoch 156, Acc(train/val): 100.00/76.44, Loss(train/val) 0.00/1.73. Took 73.49 sec\n",
      "Epoch 157, Acc(train/val): 100.00/76.29, Loss(train/val) 0.00/1.73. Took 73.40 sec\n",
      "Epoch 158, Acc(train/val): 100.00/76.45, Loss(train/val) 0.00/1.73. Took 73.57 sec\n",
      "Epoch 159, Acc(train/val): 100.00/76.43, Loss(train/val) 0.00/1.71. Took 73.64 sec\n",
      "Epoch 160, Acc(train/val): 100.00/76.45, Loss(train/val) 0.00/1.71. Took 73.70 sec\n",
      "Epoch 161, Acc(train/val): 100.00/76.54, Loss(train/val) 0.00/1.72. Took 73.57 sec\n",
      "Epoch 162, Acc(train/val): 100.00/76.52, Loss(train/val) 0.00/1.72. Took 73.60 sec\n",
      "Epoch 163, Acc(train/val): 100.00/76.54, Loss(train/val) 0.00/1.70. Took 73.72 sec\n",
      "Epoch 164, Acc(train/val): 100.00/76.63, Loss(train/val) 0.00/1.72. Took 73.55 sec\n",
      "Epoch 165, Acc(train/val): 100.00/76.41, Loss(train/val) 0.00/1.72. Took 73.85 sec\n",
      "Epoch 166, Acc(train/val): 100.00/76.23, Loss(train/val) 0.00/1.71. Took 73.41 sec\n",
      "Epoch 167, Acc(train/val): 100.00/76.46, Loss(train/val) 0.00/1.72. Took 73.32 sec\n",
      "Epoch 168, Acc(train/val): 100.00/76.41, Loss(train/val) 0.00/1.71. Took 73.57 sec\n",
      "Epoch 169, Acc(train/val): 100.00/76.33, Loss(train/val) 0.00/1.71. Took 73.43 sec\n",
      "Epoch 170, Acc(train/val): 100.00/76.34, Loss(train/val) 0.00/1.73. Took 73.63 sec\n",
      "Epoch 171, Acc(train/val): 100.00/76.48, Loss(train/val) 0.00/1.71. Took 73.57 sec\n",
      "Epoch 172, Acc(train/val): 100.00/76.37, Loss(train/val) 0.00/1.71. Took 73.56 sec\n",
      "Epoch 173, Acc(train/val): 100.00/76.54, Loss(train/val) 0.00/1.72. Took 73.57 sec\n",
      "Epoch 174, Acc(train/val): 100.00/76.45, Loss(train/val) 0.00/1.72. Took 73.59 sec\n",
      "Epoch 175, Acc(train/val): 100.00/76.46, Loss(train/val) 0.00/1.72. Took 73.75 sec\n",
      "Epoch 176, Acc(train/val): 100.00/76.62, Loss(train/val) 0.00/1.72. Took 73.53 sec\n",
      "Epoch 177, Acc(train/val): 100.00/76.52, Loss(train/val) 0.00/1.70. Took 73.84 sec\n",
      "Epoch 178, Acc(train/val): 100.00/76.45, Loss(train/val) 0.00/1.71. Took 73.44 sec\n",
      "Epoch 179, Acc(train/val): 100.00/76.62, Loss(train/val) 0.00/1.70. Took 73.88 sec\n",
      "Epoch 180, Acc(train/val): 100.00/76.56, Loss(train/val) 0.00/1.72. Took 73.56 sec\n",
      "Epoch 181, Acc(train/val): 100.00/76.45, Loss(train/val) 0.00/1.69. Took 73.45 sec\n",
      "Epoch 182, Acc(train/val): 100.00/76.47, Loss(train/val) 0.00/1.72. Took 73.68 sec\n",
      "Epoch 183, Acc(train/val): 100.00/76.50, Loss(train/val) 0.00/1.70. Took 73.79 sec\n",
      "Epoch 184, Acc(train/val): 100.00/76.48, Loss(train/val) 0.00/1.71. Took 73.62 sec\n",
      "Epoch 185, Acc(train/val): 100.00/76.48, Loss(train/val) 0.00/1.71. Took 73.58 sec\n",
      "Epoch 186, Acc(train/val): 100.00/76.44, Loss(train/val) 0.00/1.70. Took 73.62 sec\n",
      "Epoch 187, Acc(train/val): 100.00/76.54, Loss(train/val) 0.00/1.72. Took 73.60 sec\n",
      "Epoch 188, Acc(train/val): 100.00/76.53, Loss(train/val) 0.00/1.71. Took 73.98 sec\n",
      "Epoch 189, Acc(train/val): 100.00/76.32, Loss(train/val) 0.00/1.71. Took 73.42 sec\n",
      "Epoch 190, Acc(train/val): 100.00/76.46, Loss(train/val) 0.00/1.72. Took 73.71 sec\n",
      "Epoch 191, Acc(train/val): 100.00/76.39, Loss(train/val) 0.00/1.73. Took 73.54 sec\n",
      "Epoch 192, Acc(train/val): 100.00/76.46, Loss(train/val) 0.00/1.70. Took 73.49 sec\n",
      "Epoch 193, Acc(train/val): 100.00/76.37, Loss(train/val) 0.00/1.73. Took 73.57 sec\n",
      "Epoch 194, Acc(train/val): 100.00/76.36, Loss(train/val) 0.00/1.71. Took 73.69 sec\n",
      "Epoch 195, Acc(train/val): 100.00/76.40, Loss(train/val) 0.00/1.71. Took 73.57 sec\n",
      "Epoch 196, Acc(train/val): 100.00/76.42, Loss(train/val) 0.00/1.71. Took 73.85 sec\n",
      "Epoch 197, Acc(train/val): 100.00/76.32, Loss(train/val) 0.00/1.72. Took 73.68 sec\n",
      "Epoch 198, Acc(train/val): 100.00/76.34, Loss(train/val) 0.00/1.71. Took 73.37 sec\n",
      "Epoch 199, Acc(train/val): 100.00/76.50, Loss(train/val) 0.00/1.72. Took 73.40 sec\n",
      "Epoch 200, Acc(train/val): 100.00/76.42, Loss(train/val) 0.00/1.72. Took 73.61 sec\n",
      "Epoch 201, Acc(train/val): 100.00/76.40, Loss(train/val) 0.00/1.70. Took 73.42 sec\n",
      "Epoch 202, Acc(train/val): 100.00/76.56, Loss(train/val) 0.00/1.72. Took 73.67 sec\n",
      "Epoch 203, Acc(train/val): 100.00/76.29, Loss(train/val) 0.00/1.72. Took 73.45 sec\n",
      "Epoch 204, Acc(train/val): 100.00/76.44, Loss(train/val) 0.00/1.72. Took 73.74 sec\n",
      "Epoch 205, Acc(train/val): 100.00/76.54, Loss(train/val) 0.00/1.71. Took 73.58 sec\n",
      "Epoch 206, Acc(train/val): 100.00/76.39, Loss(train/val) 0.00/1.71. Took 73.38 sec\n",
      "Epoch 207, Acc(train/val): 100.00/76.48, Loss(train/val) 0.00/1.71. Took 73.56 sec\n",
      "Epoch 208, Acc(train/val): 100.00/76.53, Loss(train/val) 0.00/1.72. Took 73.69 sec\n",
      "Epoch 209, Acc(train/val): 100.00/76.51, Loss(train/val) 0.00/1.71. Took 73.61 sec\n",
      "Epoch 210, Acc(train/val): 100.00/76.38, Loss(train/val) 0.00/1.73. Took 73.79 sec\n",
      "Epoch 211, Acc(train/val): 100.00/76.44, Loss(train/val) 0.00/1.72. Took 73.62 sec\n",
      "Epoch 212, Acc(train/val): 100.00/76.35, Loss(train/val) 0.00/1.71. Took 74.00 sec\n",
      "Epoch 213, Acc(train/val): 100.00/76.36, Loss(train/val) 0.00/1.71. Took 73.68 sec\n",
      "Epoch 214, Acc(train/val): 100.00/76.32, Loss(train/val) 0.00/1.71. Took 73.87 sec\n",
      "Epoch 215, Acc(train/val): 100.00/76.41, Loss(train/val) 0.00/1.71. Took 73.77 sec\n",
      "Epoch 216, Acc(train/val): 100.00/76.36, Loss(train/val) 0.00/1.71. Took 73.63 sec\n",
      "Epoch 217, Acc(train/val): 100.00/76.49, Loss(train/val) 0.00/1.71. Took 73.61 sec\n",
      "Epoch 218, Acc(train/val): 100.00/76.41, Loss(train/val) 0.00/1.71. Took 73.53 sec\n",
      "Epoch 219, Acc(train/val): 100.00/76.44, Loss(train/val) 0.00/1.72. Took 73.46 sec\n",
      "Epoch 220, Acc(train/val): 100.00/76.32, Loss(train/val) 0.00/1.71. Took 73.76 sec\n",
      "Epoch 221, Acc(train/val): 100.00/76.38, Loss(train/val) 0.00/1.71. Took 73.41 sec\n",
      "Epoch 222, Acc(train/val): 100.00/76.46, Loss(train/val) 0.00/1.71. Took 73.89 sec\n",
      "Epoch 223, Acc(train/val): 100.00/76.58, Loss(train/val) 0.00/1.72. Took 73.72 sec\n",
      "Epoch 224, Acc(train/val): 100.00/76.46, Loss(train/val) 0.00/1.71. Took 73.56 sec\n",
      "Epoch 225, Acc(train/val): 100.00/76.41, Loss(train/val) 0.00/1.71. Took 73.51 sec\n",
      "Epoch 226, Acc(train/val): 100.00/76.53, Loss(train/val) 0.00/1.71. Took 73.62 sec\n",
      "Epoch 227, Acc(train/val): 100.00/76.35, Loss(train/val) 0.00/1.71. Took 73.52 sec\n",
      "Epoch 228, Acc(train/val): 100.00/76.33, Loss(train/val) 0.00/1.71. Took 73.81 sec\n",
      "Epoch 229, Acc(train/val): 100.00/76.57, Loss(train/val) 0.00/1.71. Took 73.70 sec\n",
      "Epoch 230, Acc(train/val): 100.00/76.55, Loss(train/val) 0.00/1.71. Took 73.66 sec\n",
      "Epoch 231, Acc(train/val): 100.00/76.63, Loss(train/val) 0.00/1.70. Took 73.47 sec\n",
      "Epoch 232, Acc(train/val): 100.00/76.36, Loss(train/val) 0.00/1.70. Took 73.62 sec\n",
      "Epoch 233, Acc(train/val): 100.00/76.52, Loss(train/val) 0.00/1.71. Took 73.42 sec\n",
      "Epoch 234, Acc(train/val): 100.00/76.48, Loss(train/val) 0.00/1.71. Took 73.71 sec\n",
      "Epoch 235, Acc(train/val): 100.00/76.41, Loss(train/val) 0.00/1.69. Took 73.88 sec\n",
      "Epoch 236, Acc(train/val): 100.00/76.45, Loss(train/val) 0.00/1.71. Took 73.39 sec\n",
      "Epoch 237, Acc(train/val): 100.00/76.51, Loss(train/val) 0.00/1.70. Took 73.50 sec\n",
      "Epoch 238, Acc(train/val): 100.00/76.47, Loss(train/val) 0.00/1.72. Took 73.37 sec\n",
      "Epoch 239, Acc(train/val): 100.00/76.42, Loss(train/val) 0.00/1.72. Took 73.46 sec\n",
      "Epoch 240, Acc(train/val): 100.00/76.34, Loss(train/val) 0.00/1.72. Took 73.57 sec\n",
      "Epoch 241, Acc(train/val): 100.00/76.37, Loss(train/val) 0.00/1.71. Took 73.76 sec\n",
      "Epoch 242, Acc(train/val): 100.00/76.41, Loss(train/val) 0.00/1.71. Took 73.23 sec\n",
      "Epoch 243, Acc(train/val): 100.00/76.60, Loss(train/val) 0.00/1.72. Took 73.66 sec\n",
      "Epoch 244, Acc(train/val): 100.00/76.48, Loss(train/val) 0.00/1.71. Took 73.59 sec\n",
      "Epoch 245, Acc(train/val): 100.00/76.70, Loss(train/val) 0.00/1.69. Took 73.53 sec\n",
      "Epoch 246, Acc(train/val): 100.00/76.52, Loss(train/val) 0.00/1.72. Took 73.76 sec\n",
      "Epoch 247, Acc(train/val): 100.00/76.55, Loss(train/val) 0.00/1.72. Took 73.46 sec\n",
      "Epoch 248, Acc(train/val): 100.00/76.58, Loss(train/val) 0.00/1.70. Took 73.67 sec\n",
      "Epoch 249, Acc(train/val): 100.00/76.51, Loss(train/val) 0.00/1.71. Took 73.34 sec\n",
      "Epoch 250, Acc(train/val): 100.00/76.59, Loss(train/val) 0.00/1.70. Took 73.33 sec\n",
      "Epoch 251, Acc(train/val): 100.00/76.42, Loss(train/val) 0.00/1.71. Took 73.48 sec\n",
      "Epoch 252, Acc(train/val): 100.00/76.34, Loss(train/val) 0.00/1.70. Took 73.58 sec\n",
      "Epoch 253, Acc(train/val): 100.00/76.51, Loss(train/val) 0.00/1.69. Took 73.28 sec\n",
      "Epoch 254, Acc(train/val): 100.00/76.41, Loss(train/val) 0.00/1.70. Took 73.49 sec\n",
      "Epoch 255, Acc(train/val): 100.00/76.50, Loss(train/val) 0.00/1.70. Took 73.59 sec\n",
      "Epoch 256, Acc(train/val): 100.00/76.61, Loss(train/val) 0.00/1.71. Took 73.75 sec\n",
      "Epoch 257, Acc(train/val): 100.00/76.58, Loss(train/val) 0.00/1.71. Took 73.79 sec\n",
      "Epoch 258, Acc(train/val): 100.00/76.63, Loss(train/val) 0.00/1.70. Took 73.88 sec\n",
      "Epoch 259, Acc(train/val): 100.00/76.53, Loss(train/val) 0.00/1.70. Took 73.65 sec\n",
      "Epoch 260, Acc(train/val): 100.00/76.44, Loss(train/val) 0.00/1.70. Took 73.45 sec\n",
      "Epoch 261, Acc(train/val): 100.00/76.46, Loss(train/val) 0.00/1.71. Took 73.50 sec\n",
      "Epoch 262, Acc(train/val): 100.00/76.43, Loss(train/val) 0.00/1.71. Took 73.89 sec\n",
      "Epoch 263, Acc(train/val): 100.00/76.43, Loss(train/val) 0.00/1.70. Took 73.56 sec\n",
      "Epoch 264, Acc(train/val): 100.00/76.46, Loss(train/val) 0.00/1.70. Took 73.79 sec\n",
      "Epoch 265, Acc(train/val): 100.00/76.59, Loss(train/val) 0.00/1.70. Took 73.46 sec\n",
      "Epoch 266, Acc(train/val): 100.00/76.41, Loss(train/val) 0.00/1.69. Took 73.76 sec\n",
      "Epoch 267, Acc(train/val): 100.00/76.52, Loss(train/val) 0.00/1.72. Took 73.60 sec\n",
      "Epoch 268, Acc(train/val): 100.00/76.52, Loss(train/val) 0.00/1.71. Took 73.56 sec\n",
      "Epoch 269, Acc(train/val): 100.00/76.51, Loss(train/val) 0.00/1.69. Took 73.60 sec\n",
      "Epoch 270, Acc(train/val): 100.00/76.63, Loss(train/val) 0.00/1.70. Took 73.66 sec\n",
      "Epoch 271, Acc(train/val): 100.00/76.56, Loss(train/val) 0.00/1.70. Took 73.55 sec\n",
      "Epoch 272, Acc(train/val): 100.00/76.52, Loss(train/val) 0.00/1.70. Took 73.69 sec\n",
      "Epoch 273, Acc(train/val): 100.00/76.62, Loss(train/val) 0.00/1.72. Took 73.54 sec\n",
      "Epoch 274, Acc(train/val): 100.00/76.46, Loss(train/val) 0.00/1.70. Took 73.76 sec\n",
      "Epoch 275, Acc(train/val): 100.00/76.52, Loss(train/val) 0.00/1.71. Took 73.44 sec\n",
      "Epoch 276, Acc(train/val): 100.00/76.34, Loss(train/val) 0.00/1.70. Took 73.44 sec\n",
      "Epoch 277, Acc(train/val): 100.00/76.54, Loss(train/val) 0.00/1.71. Took 73.63 sec\n",
      "Epoch 278, Acc(train/val): 100.00/76.68, Loss(train/val) 0.00/1.69. Took 73.71 sec\n",
      "Epoch 279, Acc(train/val): 100.00/76.58, Loss(train/val) 0.00/1.69. Took 73.49 sec\n",
      "Epoch 280, Acc(train/val): 100.00/76.55, Loss(train/val) 0.00/1.70. Took 73.58 sec\n",
      "Epoch 281, Acc(train/val): 100.00/76.46, Loss(train/val) 0.00/1.70. Took 73.51 sec\n",
      "Epoch 282, Acc(train/val): 100.00/76.70, Loss(train/val) 0.00/1.70. Took 73.79 sec\n",
      "Epoch 283, Acc(train/val): 100.00/76.60, Loss(train/val) 0.00/1.69. Took 73.51 sec\n",
      "Epoch 284, Acc(train/val): 100.00/76.58, Loss(train/val) 0.00/1.71. Took 73.47 sec\n",
      "Epoch 285, Acc(train/val): 100.00/76.60, Loss(train/val) 0.00/1.70. Took 73.48 sec\n",
      "Epoch 286, Acc(train/val): 100.00/76.51, Loss(train/val) 0.00/1.70. Took 73.61 sec\n",
      "Epoch 287, Acc(train/val): 100.00/76.42, Loss(train/val) 0.00/1.70. Took 73.55 sec\n",
      "Epoch 288, Acc(train/val): 100.00/76.63, Loss(train/val) 0.00/1.69. Took 73.50 sec\n",
      "Epoch 289, Acc(train/val): 100.00/76.54, Loss(train/val) 0.00/1.71. Took 73.47 sec\n",
      "Epoch 290, Acc(train/val): 100.00/76.61, Loss(train/val) 0.00/1.70. Took 73.64 sec\n",
      "Epoch 291, Acc(train/val): 100.00/76.48, Loss(train/val) 0.00/1.71. Took 73.54 sec\n",
      "Epoch 292, Acc(train/val): 100.00/76.50, Loss(train/val) 0.00/1.70. Took 73.45 sec\n",
      "Epoch 293, Acc(train/val): 100.00/76.53, Loss(train/val) 0.00/1.70. Took 73.86 sec\n",
      "Epoch 294, Acc(train/val): 100.00/76.63, Loss(train/val) 0.00/1.71. Took 73.81 sec\n",
      "Epoch 295, Acc(train/val): 100.00/76.59, Loss(train/val) 0.00/1.70. Took 73.58 sec\n",
      "Epoch 296, Acc(train/val): 100.00/76.44, Loss(train/val) 0.00/1.69. Took 73.79 sec\n",
      "Epoch 297, Acc(train/val): 100.00/76.64, Loss(train/val) 0.00/1.70. Took 73.83 sec\n",
      "Epoch 298, Acc(train/val): 100.00/76.37, Loss(train/val) 0.00/1.71. Took 73.54 sec\n",
      "Epoch 299, Acc(train/val): 100.00/76.60, Loss(train/val) 0.00/1.69. Took 73.57 sec\n",
      "Accuracy of plane : 76 %\n",
      "Accuracy of   car : 90 %\n",
      "Accuracy of  bird : 70 %\n",
      "Accuracy of   cat : 50 %\n",
      "Accuracy of  deer : 74 %\n",
      "Accuracy of   dog : 62 %\n",
      "Accuracy of  frog : 83 %\n",
      "Accuracy of horse : 87 %\n",
      "Accuracy of  ship : 87 %\n",
      "Accuracy of truck : 79 %\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 275.375x216 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQcAAADXCAYAAAAX4pQ8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAdEklEQVR4nO3deZxcVZ338c+3l+whCSFAIBsR1ADKkhgSgiNCUIyj6IiyiSCO+MzoM/LoCx9AH43zYmZQRwbHYQRUFiUgyDIgi7KvGkInBgyECARCCAlJICuEpJff88c51al0bnVXd9e9VTf9e79eRd/l1L3nVqhfnd+5954rM8M55zqqq3YFnHO1yYODcy6RBwfnXCIPDs65RB4cnHOJPDg45xJ5cKgASfWSNksa14P37i+pT51PlvSYpDNLrLtQ0tXZ1sgl6ZPBIX6RC682SVuK5k/r7vbMrNXMhpjZK2nUt5ZIOl3SiwnL+0laK+n4atTLVV6fDA7xizzEzIYArwCfKFo2p2N5SQ3Z17Jm3QyMknRUh+WzgG3AvdlXyaWhTwaHrsSm7Q2Srpe0Cfi8pOmS5kpaL2mlpP+U1BjLN0gySRPi/LVx/d2SNkn6k6T9ytz3GEl3SHpT0vOSzipaN03SAkkbJb0u6Udx+SBJ10l6I9ZvnqQ9Erb9HUm/6bDsUkkXx+kvSXo51nmppJM7bsPM3gZuAr7QYdUXgGvNrFXSSEl3SVojaZ2k30nat5zjT6jzpyQ9E4/rAUnvKVp3gaTX4ufxnKSjO/ucXDeZWZ9+AS8DMzssu5DwK/gJQgAdCHwAOAJoACYCfwW+Fss3AAZMiPPXAmuBKUAjcAPhi5O0//3DP0P7/OPAT4EBwOFxOx+K654ETonTQ4Ej4vRXgf+J9ayP+x2SsK+JwGZgcFG9V8fyuwEbgAPiutHAgSXq/CFgPTAgzo8AtgIHx/lRwKdjfXYDbgFuKnr/Y8CZJbZ9IXB1nJ4U63tM/BwviJ97I3AQsAzYO5bdD5jY2efkr+69vOVQ2mNm9jszazOzLWb2pJk9YWYtZrYUuILwJSnlJjNrMrNmYA5waFc7jK2LqcB5ZvaOmS0ArgJOj0WagQMkjTSzTWb2RNHyPYD9LfR/NJnZ5o7bj/VeBJwQFx0HrDezpkIR4GBJA8xspZk9W6KqjwBvAp+M8ycDi8xsUdzPGjO7NX5uG4F/pfPPqpSTgdvN7IH4OV5ECDZHAC2EAHqQpAYzeykeX+HzSPqcXDd4cChtefGMpPdKulPSKkkbgX8mfCFLWVU0/TYwpIx97gOsNbO3ipYtAwpN8i8CBwJLYuowKy6/GrgPuFHSCkkXddJPch1wSpw+lRC4iF/iUwitkFUxtXl30gYs/CT/mu2pxenANYX1kgZL+oWkV+Jn9QCdf1al7EM4/sJ+24BXgX3NbAnwTcK/w+qYAu4di5b6nFw3eHAorePpxcsJv7r7m9luwHcBVXifrwF7SBpctGwcsALAzJaY2cnAnsCPgZvjr/w2M5ttZpOAowhN+lJnXW4AZkoaQ2hBXFdYYWZ3m9lMQkrxAuGYS/kV8BFJRxLSkuuL1n2L0MyfGj+rY8o7/J28BowvzEiqA8aw/fO41sxmxH3VA/8Wlyd+Tj2sQ5/lwaF8Qwk5+VuSJgFfqfQOzOwloAn4V0n9JR1K+BWcA+2nEfeIv6AbCAGsTdIxkg6OX56NhGZ1a4l9vE7I+a8ClpjZ83HboyV9QtIgQn/LW6W2EbfzIvAEIbjcbWZrilYPJbSW1kkaSQikPXEj8ElJR8fO33OBTcATkiZJ+rCk/sCW+GqNx5L4OfWwDn2WB4fyfRM4g/A/5+WEX+A0nAQcQEhLbgIuMLMH47pZwOJ4BuXfgZPMbBuh+X0LITA8Q0gxru+44SLXATMpajUQfnnPBVYCbwBHAl/roq7XEH7Zf9Vh+cXAsLidPwJ3d7GdRGb2DOEz/xmwBjge+GTsf+gP/JDQYbuK0Cn6nfjWUp+T6waF9NE553bkLQfnXCIPDs65RB4cnHOJPDg45xJ5cHDOJfLg4JxL5MHBOZfIg4NzLlFNBYfjjz/eCJe6+stftfDq02oqOKxdu7baVXDORTUVHJxztcODg3MukQcH51yifASH5fPg0R+Hv865TNT+kOvL58FVs6CtBRoGwBm3w9ip1a6Vc7u82m85vPwotDUDBq3bwrxzLnW1HxwmfBAUq1nfL8w751JX+8Fh7FQYOw2G7OUphXMZqv0+B4BBu8M7GzwwOJeh2m85AEhgPniwc1nKR3BA+KXuzmUrH8FBdeCjZDuXqVT7HCS9THjOQyvQYmZTerghTyucy1gWHZIfNrNe3m7paYVzWfO0wjmXKO3gYMA9kuZLOrvHW/G0wrnMpZ1WzDCz1yTtCdwr6Tkze6S4QAwaZwOMGzeuxGY8rXAua6m2HMzstfh3NXArsNNVTGZ2hZlNMbMpo0aNSt6QpxXOZS614CBpsKShhWngI8CiHm7Mg4NzGUszrdgLuFVSYT/Xmdnve7YpTyucy1pqwcHMlgKHVGRjnlY4l7mcnMrEz1Y4l7F8BAdPK5zLXD6Cg6cVzmUuJ8HBL4JyLmv5CA6eVjiXuXwEB08rnMtcToKDpxXOZS0fwcHTCucyl4/g4GmFc5nLSXDweyucy1o+goOnFc5lLh/BwdMK5zKXk+DgZyucy1o+ggPgaYVz2cpHcPC0wrnM5SQ4eFrhXNZSDw6S6iX9WdIdvdgKnlY4l60sWg5fBxb3agueVjiXuVSDg6QxwMeBX/RyQ55WOJextFsOlwDfAnr5zfa0wrmspTk0/d8Cq81sfhflzpbUJKlpzZo1JQp5WuFc1tJsOcwAPhmftP0b4BhJ13YsVN5Dbbzl4FzWUgsOZna+mY0xswnAycADZvb5nm1NhY1WqHbOua7k5DqHWE0PDs5lJu0H6QJgZg8BD/V4Ayq0HNrISzxzLu9y8k2LwcH7HZzLTD6Cg7zPwbms5Sw4+IVQzmUlJ8GhUE1vOTiXlXwEB7zl4FzW8hEcvM/BuczlJDh4WuFc1vIRHDytcC5z+QgOnlY4l7mcBAdPK5zLWj6Cg9945Vzm8hEcPK1wLnM5CQ6eVjiXtXwEhwI/W+FcZvIRHDytcDknabikf6x2PbojJ8HB0wqXe8MBDw4AkgZImifpKUnPSPp+L7YW/nha4fLrIuBdkhZK+q2kEworJM2R9ElJZ0q6TdLvJS2R9L2iMp+P36eFki6XVJ92hdNsOWwFjjGzQ4BDgeMlTevRljytcPl3HvCimR0K/BfwRQBJw4AjgbtiuanAaYTvzGclTZE0CTgJmBHf3xrLpCq1YeLMzIDNcbYxvnr27fa0wu1CzOxhSZdK2hP4O+BmM2tR+BG818zeAJB0C3AU0AJMBp6MZQYCq9OuZ6pjSMamz3xgf+BSM3uih1sKfzytcLuOXxN+/U8Gzipa3vEX0AhfgGvM7PyM6gak3CFpZq2xGTQGmCrp4I5lynuojacVLvc2AUOL5q8GzgEws2eKlh8naXdJA4FPAY8D9wMnxpYGcf34tCtcVnCQ9HVJuyn4paQFkj5S7k7MbD1h9OnjE9aV8VAbTytcvsVU4XFJiyT9yMxeJzxg+qoORR8jtCoWEtKNJjN7FvgOcI+kp4F7gdFp17nctOIsM/uJpI8CowidKVcB95R6g6RRQLOZrY9RcCbwg55V09MKl39mdmphWtIg4ADg+g7FVpvZ1xLeewNwQ7o13FG5aUVhbPhZwFVm9lTRslJGAw/GSPckoaPljh7V0tMKtwuRNBN4DvipmW2odn1KKbflMF/SPcB+wPmShtLFk7PN7GngsF7WL1A+rtVyrhxmdh8wLmH51YS+iJpQbnD4EuG861Ize1vS7sTztNnwtMK5rJX7kzwdWBL7Dz5P6BzJrjnkaYVzmSs3OPwMeFvSIcC3gGXAr1KrVUfyx+E5l7Vyg0NLvOLxBOAnZvYTdjxnmzJPK5zLWrnBYZOk84HTgTvjlY+N6VWrA08rXB/U09u8Jd0laXhv919ucDiJcCPVWWa2CtgX+FFvd142vwjK9U2Jt3l3dUemmc2KFx72SllnK8xslaQ5wAck/S0wz8yy63PwtMLlxITz7pwOHA089PJFH/9TLzfXfps30Ey4kXEl4czhgZL+BxgLDCCk+1cASHoZmAIMAe4mXHV5JLACOMHMtpSz87KCg6TPEVoKDxG+qT+VdK6Z3VTeMfaSpxWuyiacd+clhC9lZ3YDDiG0yNsmnHfnU8DGTsovfPmij5/TyfrzgIPN7FBJRwN3xvmX4vqzzOzNeAXyk5JuLtzRWeQA4BQz+7KkG4HPANd2cRxA+dc5fBv4gJmthvZLo+8DMgoOnla4XBjO9lRdcb6z4NBd84oCA8A/Sfp0nB5LCAQdg8NLZrYwTs8HJpS7s3KDQ10hMERvkOkQc55WuOrq4hceaE8p7id01jcDp1UgtSj2VmEitiRmAtPjhYkPEdKLjrYWTbcSxoIoS7lf8N9L+kMcxupMQvPmri7eUzmeVrgciIHgWOC7wLEVCAwdb/MuNgxYFwPDe4GejbLWiXI7JM+V9BlgBuFn/Aozu7XSlSnJ0wqXEzEgVKS1YGZvSHpc0iJgC/B60erfA/8r3ti4BJhbiX0WK3skKDO7Gbi50hUoj7ccXN9UfJt3h+VbgY+VWDchTq4FDi5a/u/d2XenwUHSJpJ/rhX2Zbt1Z2c95mmFc5nrNDiYWYaXSHfC761wLnM5GSjBz1Y4l7V8BAdPK5zLXJpPvBor6UFJi+MTr77e0239dfXbACxZVcnrSTpY9ie47X/DHf8Hls9Lbz/O5USaLYcW4JtmNolwDvarkg7s7kbmL1vHhXcuBuDCOxYxf9m6ytYSQjC4ahb8+VfQdCVc+VFourry+3EuR1ILDma20swWxOlNhGG49+3uduYufYPmtpBOtLa2MXdpx6tDK+DlR9lhSExrgzvOgZu/DL/+tAcKlwuSNnddqnypPvGqQNIEwmCzOz3xStLZwNkA48btNOYm0yaO5NEYwxrrxbSJIytfwQkfTFho8Jcbw+SLD8C6l+C4XjwL2LmcSb1DUtIQwsVT55jZTp0GXT3UZvL4ERw+bncAvj3rvUweP6KyFVw+Dx75Ydfl/vgT74twXZs9bDqzh53P7GHTe7spST8oHuxF0mxJ35N0f3yw1F+Kn9ZdaWk/K7OREBjmmNktPd3OnsMGwCp4956Duy68fF5IEyZ8EMZO3XHd8/fBqqfgnY3w0iPQOCB0RJZz/YQZPP4TOHlOj47B5dzsYd2+ZZvZw7q8ZZvZGzq7oes3wCXAf8f5zxGeGvcfZrZR0h7AXEm3x2EcKyq14KDwOOBfAovN7OLebGtAY6immXX+JJ2XHoVfnxC+yPX94fiL4IV7YNOq8P1/bX5vqgFL7g7Bp2PQcS6o6C3bZvZnSXtK2ofwpLl1hMFe/kPS3xA6yvYF9gJW9abiSdJsOcwgjDn5lziSDcAFZtbtuzn79wvV3NbSQv/OCjZdCW2tYbplK9zRk7OnomRLwtrgqes8OPRFnf/CxzLDdrplm9kbensT1k3AicDehJbEaYRAMdnMmuOoT0m3avdaasHBzB6j60fmlWVAYxgy753m1s6Dw4gJ26c7+Y6XNP5IeN9JcOc3wFoTChg0XQVrlsDM79dGkChOoyBMDxwJqxbCmr/C+uXQbxAc8Q+w14E7rkew9yHbpw85JfmYls8LQbFj+f67waqnYdIJYduFMoXtFOo2cCRseSM51Uvjc6jWv8vsDX9i9rBjicPEVSAwQAgIPwf2AD5ESC1Wx8DwYSC1p21ncrait/o3hoGu65+5BYaPLP2PvyVeAzFgBDQOgk0ryt/JvpPhi3dvn7/zG6GloLqdA8WyP8Ivj4Mhe8GQPWHLBmjdBg39YcBu2+eLtbVCXcK4oNYWrwAtukS8sF8J2trC/lUXykjxMnKFfWwt89lC5bSimq6E/sOgvl+h0tDaClu7GKv0xQd23k7jEGhOOLPWf3jo64Htx1o4Xgh/21qgrqHrxyC2tWwvv+XN7csHjghpZdgJJX+jGvrD3u+DGV+vXEAJAaFiA7yY2TPx8ZMrzGxlHMv1d5KaCE/ifq5S++ooF8Fh1JYXARj83G/hhd/BGbfv/I+5fB7MvyZMv7MuvLrjsC9sn55y5o6/sqVaEptfD69dSbnBpitJgQFCoNmavKpitnTj3379MvjrH+CLd9VGSzCBmb2vaHot4Ql0SeWGVHK/uQgOu28MV0gKw1q2opcf3fkf8qnr6eLZvtupDo78JxixHyy+LTSLp5y5Y5mxU3fcxx3n4HeF7qLamsMPQY0Gh2rJRXBY19xv+6WV1sbLWwbsOEpmcashycRjQsty7/eHZn9xXtoxKCQptCTu+15IKdyupa6xxIVwfVsugsPr2/pxkIV0uxWxcuWKHYPDwjklOhCB930OPvPz3ldi7NTQJ7F8Hjx+CSxvgrdW096a6DcUtm3ePr9D3pui4n4OKUy3bIPBe8Co94TOwdefhbn/DS3v7Lge4K2126fXL9+5r6Swj2Fjdi7f8g7s/i5Y+VSYLpQpbKdQt5Zt0NAvuS8mjc+h3H2k0eewC8lFcNjz/TNpXvEL+tFKCw2MOPCYHQus7qRP5u21la3M2Klw8nVhumMPeS30mCcZO7W8FpJzRXIRHN437SNccvdnOYff8PreR/PevePodIVTbJ1d1jwptatLd+6X6DjvXI7lIjjMX7aONS2DoBHGrLqPtqsfpe5jP4C7vhlOY7VT+IXc+9DSHY3OubLkIjjMXfoG4xWuDq2X0dbaDM/e1iEwAFgIDFPO9KDgXC/lYpi4aRNH8qiFe15aTbSpHta/klx4SwrjPTjXB+UiOEweP4LRh36UrdbAkraxocXw5gs7F1S9n5JyrkJyERwA+jc2sIHB7K6N1Cdd7KQ6+PjF3iHoXIXkos8B4O9GrWAPNiIlXKU4/sjauRHKuV1EbloOh7UtAgyp6Bk3EFoM+8/0wOBcheUmODw34BCKh3ppbz/4pa/OpSI3weH+zRO4p3Vy+7wK/z3sVG81OJeCNB9qc6Wk1fHx4b02beJIFrMfcZT60HKoq4dDEh9C7JzrpTRbDlcTBsOsiMnjR7DXIR9lK/1oRZgaYNaPvdXgXErSHCbukfi8ioppHfMBTltwAdPrFzNfB3HuqBOY3PXbnHM9kJtTmQCvrd/CAns3C1reTb3CZdUVf46Fcw6ogQ5JSWdLapLUtGbNmk7Lzpy0V/t0fV1KT79yzgE1EBy6euJVMUnbT2aqxKChzrmKqHpw6I65S99ov76huaWNmxe8WtX6OLcrS/NU5vWEIbrfI+lVSV/q7TanTRxJfWEEd+Cm+a8yf1k3R5l2zpUlteBgZqeY2WgzazSzMWb2y95uc/L4EXzs4NHt862tbcxd6rdoO5eGXKUVAKdPDw/4EdDYUOedks6lJFenMgGOmDiS3Qc1UlcnvnHce/xUpnMpyV1wmL9sHeu3NNNm8N3bwpXZpx4xrsq1cm7Xk7u0Yu7SN9rvr2hpM7572yLvlHQuBbkLDtMmjqS+bvs1Dq1t5p2SzqUgd8Fh8vgRfPmo/drnDRgxqF/pNzjneiR3wQFg6MDG9uk6wbq3U3rEmnN9WC6DQ3Fq0c9PZzqXilwGh8njR3D2B0Nqsd/IwSxZtanKNXJu15PL4ABg8YzF4lWbuODWv3DRXYurWyHndjG5DQ7PrNy4w/wVjy71U5rOVVBug0PxPRYAbYaf0nSugnIbHE49YhyfOnSfHZb5KU3nKie3wQHggL2Gtk8LWPTahupVxrldTK6DQzilGaYNuLFpufc7OFchuQ4Ok8eP4Oh3bx9arqXVOPXnc5l58cNc98QrVayZc/mXanCQdLykJZJekHReGvvYe9jAHea3trTxwurNfnrTuV6SWcJTqyuxYake+CtwHPAq8CRwipk9W+o9U6ZMsaampm7tZ/6ydZz4sz9S6iiGD2xgj6EDOGtGuGjq7kUr+djBoxNv856/bB2XPfwiz762ga2tbfH9/Thrxn5+W3jf1KdHMU4zOEwHZpvZR+P8+QBm9m+l3tOT4ABw0V2LueyRpd16z6B+dQxoqKcuDkrZ3NLGhi0tJcsP7V9P/3713a6bq139G+o5aPRufOVD7yo1aFCfDg5pDvayL7C8aP5V4Ig0dnTerEmMGzmYKx9byvL1W9ja3Nble97e1sbb27ouV7Bpayubtrb2ppquBq1Yt4UHl6zmN2dP91HFOkizzyEp6u7UTOnOQ206c+oR47jvm0dz3d9Paz+D4Vw5mlt9TJAkaX6NXgXGFs2PAV7rWKg7D7Upx+TxI7jxK0dy3IF7se/wAQzu37NUYPjAhh6/1+VLY70/PS1Jmn0ODYQOyWOBFYQOyVPN7JlS7+lpn0NXrnviFa58bClIHLzPbixasYH17zTvVC4pBy28N6m8yzfvc+hcasEBQNIs4BKgHrjSzP6ls/JpBQfneqhPB4dUR582s7uAu9Lch3MuHd5155xL5MHBOZco1T6H7pK0BlhWYvUewNoMq5MWP47a0tlxrDWz47OsTC2pqeDQGUlNZjal2vXoLT+O2rKrHEcaPK1wziXy4OCcS5Sn4HBFtStQIX4ctWVXOY6Ky02fg3MuW3lqOTjnMpSL4JDFiFKVIulKSaslLSpatrukeyU9H/+OiMsl6T/jcT0t6fDq1Xw7SWMlPShpsaRnJH09Ls/VcQBIGiBpnqSn4rF8Py7fT9IT8VhukNQvLu8f51+I6ydUs/5VZWY1/SLcl/EiMBHoBzwFHFjtenVS378BDgcWFS37IXBenD4P+EGcngXcTbiGfxrwRLXrH+s1Gjg8Tg8l3EB3YN6OI9ZNwJA43Qg8Eet4I3ByXH4Z8A9x+h+By+L0ycAN1T6Gqn121a5AGf+404E/FM2fD5xf7Xp1UecJHYLDEmB0nB4NLInTlxOGztupXC29gNsIw/3l/TgGAQsIgw6tBRo6/j8G/AGYHqcbYjlVu+7VeOUhrUgaUWrfKtWlp/Yys5UA8e+ecXnNH1tsVh9G+MXN5XFIqpe0EFgN3Etoia43s8K4gMX1bT+WuH4D0CcHe8hDcChrRKmcquljkzQEuBk4x8w2dlY0YVnNHIeZtZrZoYQBh6YCk5KKxb81fSxZykNwKGtEqRr3uqTRAPHv6ri8Zo9NUiMhMMwxs1vi4twdRzEzWw88ROhzGB4HJIId69t+LHH9MODNbGtaG/IQHJ4EDoi9y/0InUS3V7lO3XU7cEacPoOQwxeWfyH29k8DNhSa7dUkScAvgcVmdnHRqlwdB4CkUZKGx+mBwExgMfAgcGIs1vFYCsd4IvCAxQ6IPqfanR5ldiTNIvSYvwh8u9r16aKu1wMrgWbCr9CXCDnr/cDz8e/usayAS+Nx/QWYUu36x3odRWhKPw0sjK9ZeTuOWLf3A3+Ox7II+G5cPhGYB7wA/BboH5cPiPMvxPUTq30M1Xr5FZLOuUR5SCucc1XgwcE5l8iDg3MukQcH51wiDw7OuUQeHPogSUdLuqPa9XC1zYODcy6RB4caJunzcSyChZIujzcQbZb0Y0kLJN0vaVQse6ikuXE8hVuLxlrYX9J9cTyDBZLeFTc/RNJNkp6TNCdeFelcOw8ONUrSJOAkYIaFm4ZagdOAwcACMzsceBj4XnzLr4D/a2bvJ1ylWFg+B7jUzA4BjiRcvQnhTstzCOM0TARmpH5QLldSfVam65VjgcnAk/FHfSDhRqc24IZY5lrgFknDgOFm9nBcfg3wW0lDgX3N7FYAM3sHIG5vnpm9GucXEsageCz9w3J54cGhdgm4xszO32Gh9P86lOvs+vfOUoWtRdOt+P8LrgNPK2rX/cCJkvaE9vEbxxP+zQp3E54KPGZmG4B1kj4Yl58OPGxhDIZXJX0qbqO/pEGZHoXLLf+1qFFm9qyk7wD3SKoj3OX5VeAt4CBJ8wmjFJ0U33IGcFn88i8FvhiXnw5cLumf4zY+m+FhuBzzuzJzRtJmMxtS7Xq4XZ+nFc65RN5ycM4l8paDcy6RBwfnXCIPDs65RB4cnHOJPDg45xJ5cHDOJfr/CIs44LB6HJ4AAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 275.375x216 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.expansion = 4\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != 4 * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, 4 * planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(4 * planes)\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# resnet-50\n",
    "class Resnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Resnet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        # self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)    # for cifar-10\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)   # for cifar-10\n",
    "        self.layer1 = self.residual_block(64, 3, stride=1)\n",
    "        self.layer2 = self.residual_block(128, 4, stride=2)\n",
    "        self.layer3 = self.residual_block(256, 6, stride=2)\n",
    "        self.layer4 = self.residual_block(512, 3, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(2048, 10)\n",
    "\n",
    "\n",
    "    def residual_block(self, planes, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(Bottleneck(self.inplanes , planes, stride))\n",
    "        self.inplanes = planes * 4\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(Bottleneck(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = \"exp1_lr\"\n",
    "#models = ['CNN', 'Resnet']\n",
    "args.model = 'Resnet'\n",
    "args.act = 'relu'\n",
    "args.l2 = 0.00001\n",
    "args.optim = 'SGD'  # 'RMSprop' #SGD, RMSprop, ADAM...\n",
    "args.lr = 1e-3\n",
    "args.epoch = 300\n",
    "\n",
    "args.train_batch_size = 256\n",
    "args.test_batch_size = 64\n",
    "\n",
    "\n",
    "#torchsummary.summary(Resnet().cuda(), (3, 32, 32))\n",
    "print(args)\n",
    "setting, result = experiment(partition, deepcopy(args))\n",
    "plot_loss_variation(result)\n",
    "plot_acc_variation(result)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: More case studies\n",
    "### 3.4.7 DenseNet\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide86.PNG?raw=true)\n",
    "\n",
    "    Vanishing Gradient 개선, Feature Propagation 강화, Feature Reuse, Parameter 수 절약이라는 이점\n",
    "\n",
    "    ResNet과 같은 연결방식은 위와 같이 residual을 더하는 방식으로 생겼다.\n",
    "    이전 레이어의 output을 다음의 레이어의 output과 합해서 더한다는 점에서, 정보들이 이후의 레이어들로 온전히 흘러가는 것을 방해할 수 있다는 약점이 있었다.\n",
    "    Layer간의 정보가 흘러가는 것을 개선하기 위해, DenseNet에서는 ResNet과는 조금 다른 연결 패턴을 제안했다.\n",
    "    이 수식에서 중요한 것은 x_0 ... x_l-1 을 더하지(sum) 않고 이어붙였다는(concatenation) 것이다.\n",
    "    DesNet 수식의 function H는 위와 같이 x가 concatenation 된 함수로 구성된다.\n",
    "    Transition layer : batch normalization -> 1x1 conv -> 2x2 average pool(stride2)\n",
    "    Composite function :  Batch Normalization -> ReLU -> 3x3 Conv Layer\n",
    "\n",
    "    dense block이 m 개의 feature-map을 포함하는 경우 다음 transition layer에서 출력 feature-map을 [θmc]개가 생성된다.\n",
    "    여기서 0 <θ≤1은 compression factor라고 한다. (θ 가 1인 경우에는 transition layer의 특징맵 개수가 변경되지 않는다)\n",
    "    θ<1 인 DenseNet을 DenseNet-C라고 칭하며, 실험에서는 θ=0.5로 설정한다.\n",
    "    또한, bottleneck layer와 θ<1 인 transition layer를 모두 사용하는 모델은 DenseNet-BC\n",
    "\n",
    "    growth_rate = 4\n",
    "    compression factor = 0.5\n",
    "    input = 6\n",
    "    [6 -> (6)+4 -> (6+4)+4 -> (6+4+4)+4 -> (6+4+4+4)+4 = 22(32x32x22)] DenseBlock x 3\n",
    "    -(transition)> (16x16x11) (16은 2x2풀링, 11은 1x1 factor 0.5)\n",
    "    마지막에 Nx8x8 -(global avg pooling)> Nx1x1 -(flatten)> FC\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(act='relu', epoch=300, exp_name='exp1_lr', l2=1e-05, lr=0.001, model='Densenet', optim='SGD', test_batch_size=64, train_batch_size=128)\n",
      "Epoch 0, Acc(train/val): 22.70/23.04, Loss(train/val) 2.06/2.22. Took 93.33 sec\n",
      "Epoch 1, Acc(train/val): 33.10/27.33, Loss(train/val) 1.77/2.11. Took 93.51 sec\n",
      "Epoch 2, Acc(train/val): 37.86/25.34, Loss(train/val) 1.64/2.34. Took 93.03 sec\n",
      "Epoch 3, Acc(train/val): 42.89/22.41, Loss(train/val) 1.54/3.08. Took 92.61 sec\n",
      "Epoch 4, Acc(train/val): 45.48/31.87, Loss(train/val) 1.48/2.08. Took 92.58 sec\n",
      "Epoch 5, Acc(train/val): 48.59/34.90, Loss(train/val) 1.41/2.21. Took 92.44 sec\n",
      "Epoch 6, Acc(train/val): 49.27/37.33, Loss(train/val) 1.39/2.05. Took 92.31 sec\n",
      "Epoch 7, Acc(train/val): 51.74/37.58, Loss(train/val) 1.33/2.22. Took 92.57 sec\n",
      "Epoch 8, Acc(train/val): 53.44/46.39, Loss(train/val) 1.29/1.72. Took 93.22 sec\n",
      "Epoch 9, Acc(train/val): 56.55/46.10, Loss(train/val) 1.21/1.81. Took 92.62 sec\n",
      "Epoch 10, Acc(train/val): 58.63/45.95, Loss(train/val) 1.15/1.84. Took 92.62 sec\n",
      "Epoch 11, Acc(train/val): 60.55/49.71, Loss(train/val) 1.10/1.64. Took 92.47 sec\n",
      "Epoch 12, Acc(train/val): 61.08/49.60, Loss(train/val) 1.07/1.71. Took 92.57 sec\n",
      "Epoch 13, Acc(train/val): 62.58/49.73, Loss(train/val) 1.05/1.78. Took 92.52 sec\n",
      "Epoch 14, Acc(train/val): 63.11/53.06, Loss(train/val) 1.03/1.60. Took 92.52 sec\n",
      "Epoch 15, Acc(train/val): 64.06/55.76, Loss(train/val) 1.00/1.48. Took 92.70 sec\n",
      "Epoch 16, Acc(train/val): 64.97/57.68, Loss(train/val) 0.97/1.36. Took 92.47 sec\n",
      "Epoch 17, Acc(train/val): 66.69/57.13, Loss(train/val) 0.93/1.39. Took 92.50 sec\n",
      "Epoch 18, Acc(train/val): 67.27/59.23, Loss(train/val) 0.91/1.32. Took 92.60 sec\n",
      "Epoch 19, Acc(train/val): 68.04/56.07, Loss(train/val) 0.89/1.46. Took 92.68 sec\n",
      "Epoch 20, Acc(train/val): 68.52/62.40, Loss(train/val) 0.88/1.19. Took 92.56 sec\n",
      "Epoch 21, Acc(train/val): 68.51/56.96, Loss(train/val) 0.87/1.37. Took 93.09 sec\n",
      "Epoch 22, Acc(train/val): 70.06/63.18, Loss(train/val) 0.84/1.20. Took 92.61 sec\n",
      "Epoch 23, Acc(train/val): 70.48/62.74, Loss(train/val) 0.82/1.24. Took 92.58 sec\n",
      "Epoch 24, Acc(train/val): 70.33/66.37, Loss(train/val) 0.82/1.06. Took 92.84 sec\n",
      "Epoch 25, Acc(train/val): 70.91/63.20, Loss(train/val) 0.82/1.17. Took 92.60 sec\n",
      "Epoch 26, Acc(train/val): 71.52/65.08, Loss(train/val) 0.80/1.10. Took 92.52 sec\n",
      "Epoch 27, Acc(train/val): 72.72/65.95, Loss(train/val) 0.77/1.08. Took 92.46 sec\n",
      "Epoch 28, Acc(train/val): 72.85/67.40, Loss(train/val) 0.76/1.03. Took 92.62 sec\n",
      "Epoch 29, Acc(train/val): 73.51/63.33, Loss(train/val) 0.75/1.22. Took 92.67 sec\n",
      "Epoch 30, Acc(train/val): 74.30/69.42, Loss(train/val) 0.73/0.95. Took 92.53 sec\n",
      "Epoch 31, Acc(train/val): 74.41/65.16, Loss(train/val) 0.72/1.11. Took 92.58 sec\n",
      "Epoch 32, Acc(train/val): 74.52/68.34, Loss(train/val) 0.71/0.99. Took 92.60 sec\n",
      "Epoch 33, Acc(train/val): 73.72/67.76, Loss(train/val) 0.73/1.01. Took 92.54 sec\n",
      "Epoch 34, Acc(train/val): 74.76/68.54, Loss(train/val) 0.70/1.00. Took 92.44 sec\n",
      "Epoch 35, Acc(train/val): 75.37/67.94, Loss(train/val) 0.69/1.01. Took 92.65 sec\n",
      "Epoch 36, Acc(train/val): 76.25/69.61, Loss(train/val) 0.67/0.95. Took 92.51 sec\n",
      "Epoch 37, Acc(train/val): 76.66/69.97, Loss(train/val) 0.66/0.97. Took 92.60 sec\n",
      "Epoch 38, Acc(train/val): 76.53/69.40, Loss(train/val) 0.66/0.96. Took 92.64 sec\n",
      "Epoch 39, Acc(train/val): 76.69/68.53, Loss(train/val) 0.65/1.02. Took 92.51 sec\n",
      "Epoch 40, Acc(train/val): 77.22/69.82, Loss(train/val) 0.64/0.96. Took 92.72 sec\n",
      "Epoch 41, Acc(train/val): 77.54/71.94, Loss(train/val) 0.63/0.87. Took 92.79 sec\n",
      "Epoch 42, Acc(train/val): 78.09/70.55, Loss(train/val) 0.62/0.93. Took 92.50 sec\n",
      "Epoch 43, Acc(train/val): 78.30/71.07, Loss(train/val) 0.61/0.92. Took 92.56 sec\n",
      "Epoch 44, Acc(train/val): 78.92/72.16, Loss(train/val) 0.60/0.89. Took 92.60 sec\n",
      "Epoch 45, Acc(train/val): 79.35/70.85, Loss(train/val) 0.58/0.97. Took 92.63 sec\n",
      "Epoch 46, Acc(train/val): 79.72/72.25, Loss(train/val) 0.57/0.89. Took 92.37 sec\n",
      "Epoch 47, Acc(train/val): 79.72/71.58, Loss(train/val) 0.58/0.91. Took 92.52 sec\n",
      "Epoch 48, Acc(train/val): 79.25/74.40, Loss(train/val) 0.58/0.82. Took 92.62 sec\n",
      "Epoch 49, Acc(train/val): 79.40/73.87, Loss(train/val) 0.58/0.84. Took 92.38 sec\n",
      "Epoch 50, Acc(train/val): 79.90/75.97, Loss(train/val) 0.57/0.75. Took 92.27 sec\n",
      "Epoch 51, Acc(train/val): 80.58/74.12, Loss(train/val) 0.55/0.83. Took 92.90 sec\n",
      "Epoch 52, Acc(train/val): 81.10/74.85, Loss(train/val) 0.54/0.80. Took 92.69 sec\n",
      "Epoch 53, Acc(train/val): 81.32/75.98, Loss(train/val) 0.53/0.75. Took 92.44 sec\n",
      "Epoch 54, Acc(train/val): 81.71/75.64, Loss(train/val) 0.52/0.80. Took 92.38 sec\n",
      "Epoch 55, Acc(train/val): 82.14/76.09, Loss(train/val) 0.51/0.77. Took 92.27 sec\n",
      "Epoch 56, Acc(train/val): 82.34/75.81, Loss(train/val) 0.50/0.80. Took 92.34 sec\n",
      "Epoch 57, Acc(train/val): 82.24/74.89, Loss(train/val) 0.51/0.84. Took 92.49 sec\n",
      "Epoch 58, Acc(train/val): 82.05/75.33, Loss(train/val) 0.51/0.83. Took 92.68 sec\n",
      "Epoch 59, Acc(train/val): 83.00/77.51, Loss(train/val) 0.49/0.74. Took 92.47 sec\n",
      "Epoch 60, Acc(train/val): 82.47/76.97, Loss(train/val) 0.49/0.77. Took 92.26 sec\n",
      "Epoch 61, Acc(train/val): 82.85/78.30, Loss(train/val) 0.49/0.72. Took 92.41 sec\n",
      "Epoch 62, Acc(train/val): 83.76/77.94, Loss(train/val) 0.46/0.71. Took 92.36 sec\n",
      "Epoch 63, Acc(train/val): 84.01/79.92, Loss(train/val) 0.45/0.64. Took 92.41 sec\n",
      "Epoch 64, Acc(train/val): 83.78/78.29, Loss(train/val) 0.46/0.70. Took 92.40 sec\n",
      "Epoch 65, Acc(train/val): 83.83/79.63, Loss(train/val) 0.46/0.66. Took 92.34 sec\n",
      "Epoch 66, Acc(train/val): 84.23/78.75, Loss(train/val) 0.45/0.71. Took 92.34 sec\n",
      "Epoch 67, Acc(train/val): 83.90/79.92, Loss(train/val) 0.46/0.66. Took 92.60 sec\n",
      "Epoch 68, Acc(train/val): 83.88/77.59, Loss(train/val) 0.46/0.76. Took 92.71 sec\n",
      "Epoch 69, Acc(train/val): 84.30/80.24, Loss(train/val) 0.44/0.64. Took 92.59 sec\n",
      "Epoch 70, Acc(train/val): 85.13/78.65, Loss(train/val) 0.42/0.70. Took 92.53 sec\n",
      "Epoch 71, Acc(train/val): 84.99/80.57, Loss(train/val) 0.43/0.63. Took 91.72 sec\n",
      "Epoch 72, Acc(train/val): 85.12/78.72, Loss(train/val) 0.42/0.74. Took 92.90 sec\n",
      "Epoch 73, Acc(train/val): 85.13/80.49, Loss(train/val) 0.42/0.63. Took 92.57 sec\n",
      "Epoch 74, Acc(train/val): 85.23/77.75, Loss(train/val) 0.42/0.78. Took 92.57 sec\n",
      "Epoch 75, Acc(train/val): 85.50/79.92, Loss(train/val) 0.41/0.65. Took 92.60 sec\n",
      "Epoch 76, Acc(train/val): 85.54/79.06, Loss(train/val) 0.41/0.72. Took 92.55 sec\n",
      "Epoch 77, Acc(train/val): 85.97/80.72, Loss(train/val) 0.40/0.61. Took 92.63 sec\n",
      "Epoch 78, Acc(train/val): 86.11/80.28, Loss(train/val) 0.39/0.68. Took 92.50 sec\n",
      "Epoch 79, Acc(train/val): 86.47/80.46, Loss(train/val) 0.39/0.65. Took 92.56 sec\n",
      "Epoch 80, Acc(train/val): 86.70/80.20, Loss(train/val) 0.38/0.68. Took 92.55 sec\n",
      "Epoch 81, Acc(train/val): 86.46/82.06, Loss(train/val) 0.38/0.59. Took 92.56 sec\n",
      "Epoch 82, Acc(train/val): 86.55/80.18, Loss(train/val) 0.38/0.68. Took 92.52 sec\n",
      "Epoch 83, Acc(train/val): 86.97/81.09, Loss(train/val) 0.37/0.64. Took 92.64 sec\n",
      "Epoch 84, Acc(train/val): 87.08/80.17, Loss(train/val) 0.36/0.69. Took 92.51 sec\n",
      "Epoch 85, Acc(train/val): 87.30/81.63, Loss(train/val) 0.36/0.61. Took 92.66 sec\n",
      "Epoch 86, Acc(train/val): 87.53/80.44, Loss(train/val) 0.36/0.68. Took 92.60 sec\n",
      "Epoch 87, Acc(train/val): 87.47/81.70, Loss(train/val) 0.35/0.62. Took 92.76 sec\n",
      "Epoch 88, Acc(train/val): 87.34/80.76, Loss(train/val) 0.36/0.67. Took 92.49 sec\n",
      "Epoch 89, Acc(train/val): 87.54/82.54, Loss(train/val) 0.35/0.61. Took 92.64 sec\n",
      "Epoch 90, Acc(train/val): 87.51/80.01, Loss(train/val) 0.35/0.70. Took 92.54 sec\n",
      "Epoch 91, Acc(train/val): 88.33/80.74, Loss(train/val) 0.33/0.71. Took 92.66 sec\n",
      "Epoch 92, Acc(train/val): 88.27/81.01, Loss(train/val) 0.33/0.66. Took 92.52 sec\n",
      "Epoch 93, Acc(train/val): 88.39/80.89, Loss(train/val) 0.33/0.69. Took 92.53 sec\n",
      "Epoch 94, Acc(train/val): 88.62/81.02, Loss(train/val) 0.33/0.67. Took 92.60 sec\n",
      "Epoch 95, Acc(train/val): 88.78/81.58, Loss(train/val) 0.32/0.66. Took 92.60 sec\n",
      "Epoch 96, Acc(train/val): 88.59/81.17, Loss(train/val) 0.32/0.68. Took 92.58 sec\n",
      "Epoch 97, Acc(train/val): 88.58/81.11, Loss(train/val) 0.32/0.69. Took 92.62 sec\n",
      "Epoch 98, Acc(train/val): 88.94/81.96, Loss(train/val) 0.31/0.65. Took 92.66 sec\n",
      "Epoch 99, Acc(train/val): 88.94/80.16, Loss(train/val) 0.31/0.73. Took 92.46 sec\n",
      "Epoch 100, Acc(train/val): 89.00/82.19, Loss(train/val) 0.31/0.64. Took 92.60 sec\n",
      "Epoch 101, Acc(train/val): 88.73/80.96, Loss(train/val) 0.32/0.71. Took 92.48 sec\n",
      "Epoch 102, Acc(train/val): 89.34/82.41, Loss(train/val) 0.30/0.64. Took 92.60 sec\n",
      "Epoch 103, Acc(train/val): 89.64/82.69, Loss(train/val) 0.29/0.63. Took 92.59 sec\n",
      "Epoch 104, Acc(train/val): 89.36/82.35, Loss(train/val) 0.30/0.63. Took 92.66 sec\n",
      "Epoch 105, Acc(train/val): 89.83/81.83, Loss(train/val) 0.29/0.68. Took 92.47 sec\n",
      "Epoch 106, Acc(train/val): 89.40/82.12, Loss(train/val) 0.29/0.65. Took 92.65 sec\n",
      "Epoch 107, Acc(train/val): 89.76/81.94, Loss(train/val) 0.29/0.67. Took 92.59 sec\n",
      "Epoch 108, Acc(train/val): 90.13/81.83, Loss(train/val) 0.28/0.68. Took 92.63 sec\n",
      "Epoch 109, Acc(train/val): 89.86/82.22, Loss(train/val) 0.28/0.65. Took 92.98 sec\n",
      "Epoch 110, Acc(train/val): 90.31/82.45, Loss(train/val) 0.27/0.63. Took 92.61 sec\n",
      "Epoch 111, Acc(train/val): 90.20/83.06, Loss(train/val) 0.27/0.63. Took 93.33 sec\n",
      "Epoch 112, Acc(train/val): 90.56/82.03, Loss(train/val) 0.27/0.69. Took 92.88 sec\n",
      "Epoch 113, Acc(train/val): 90.34/82.68, Loss(train/val) 0.27/0.68. Took 93.30 sec\n",
      "Epoch 114, Acc(train/val): 90.37/82.56, Loss(train/val) 0.27/0.64. Took 92.80 sec\n",
      "Epoch 115, Acc(train/val): 90.74/82.43, Loss(train/val) 0.26/0.68. Took 93.17 sec\n",
      "Epoch 116, Acc(train/val): 90.92/83.44, Loss(train/val) 0.26/0.64. Took 93.32 sec\n",
      "Epoch 117, Acc(train/val): 90.49/82.89, Loss(train/val) 0.26/0.65. Took 93.31 sec\n",
      "Epoch 118, Acc(train/val): 90.64/83.14, Loss(train/val) 0.26/0.67. Took 93.33 sec\n",
      "Epoch 119, Acc(train/val): 91.17/82.54, Loss(train/val) 0.25/0.66. Took 93.49 sec\n",
      "Epoch 120, Acc(train/val): 91.12/83.04, Loss(train/val) 0.25/0.68. Took 93.31 sec\n",
      "Epoch 121, Acc(train/val): 91.20/82.52, Loss(train/val) 0.24/0.67. Took 93.25 sec\n",
      "Epoch 122, Acc(train/val): 91.27/83.20, Loss(train/val) 0.25/0.66. Took 93.54 sec\n",
      "Epoch 123, Acc(train/val): 91.11/81.60, Loss(train/val) 0.25/0.73. Took 93.28 sec\n",
      "Epoch 124, Acc(train/val): 90.63/81.74, Loss(train/val) 0.26/0.71. Took 93.40 sec\n",
      "Epoch 125, Acc(train/val): 91.18/82.30, Loss(train/val) 0.25/0.70. Took 93.60 sec\n",
      "Epoch 126, Acc(train/val): 91.55/82.49, Loss(train/val) 0.23/0.70. Took 93.37 sec\n",
      "Epoch 127, Acc(train/val): 91.60/82.08, Loss(train/val) 0.24/0.72. Took 93.34 sec\n",
      "Epoch 128, Acc(train/val): 91.33/81.56, Loss(train/val) 0.24/0.76. Took 93.13 sec\n",
      "Epoch 129, Acc(train/val): 91.69/81.72, Loss(train/val) 0.24/0.73. Took 93.59 sec\n",
      "Epoch 130, Acc(train/val): 91.59/82.53, Loss(train/val) 0.23/0.71. Took 92.97 sec\n",
      "Epoch 131, Acc(train/val): 92.07/82.58, Loss(train/val) 0.22/0.69. Took 93.13 sec\n",
      "Epoch 132, Acc(train/val): 92.22/82.77, Loss(train/val) 0.22/0.70. Took 93.30 sec\n",
      "Epoch 133, Acc(train/val): 92.33/82.63, Loss(train/val) 0.22/0.71. Took 93.11 sec\n",
      "Epoch 134, Acc(train/val): 92.23/82.75, Loss(train/val) 0.22/0.72. Took 93.22 sec\n",
      "Epoch 135, Acc(train/val): 92.54/83.14, Loss(train/val) 0.21/0.68. Took 93.19 sec\n",
      "Epoch 136, Acc(train/val): 92.83/82.47, Loss(train/val) 0.20/0.71. Took 93.27 sec\n",
      "Epoch 137, Acc(train/val): 92.58/83.67, Loss(train/val) 0.21/0.67. Took 93.14 sec\n",
      "Epoch 138, Acc(train/val): 92.60/83.65, Loss(train/val) 0.20/0.67. Took 93.71 sec\n",
      "Epoch 139, Acc(train/val): 92.70/82.44, Loss(train/val) 0.20/0.73. Took 93.15 sec\n",
      "Epoch 140, Acc(train/val): 92.77/83.21, Loss(train/val) 0.20/0.70. Took 92.91 sec\n",
      "Epoch 141, Acc(train/val): 92.72/81.67, Loss(train/val) 0.20/0.78. Took 93.48 sec\n",
      "Epoch 142, Acc(train/val): 92.54/81.40, Loss(train/val) 0.21/0.82. Took 93.23 sec\n",
      "Epoch 143, Acc(train/val): 92.83/82.51, Loss(train/val) 0.20/0.75. Took 93.08 sec\n",
      "Epoch 144, Acc(train/val): 93.07/82.27, Loss(train/val) 0.19/0.74. Took 93.10 sec\n",
      "Epoch 145, Acc(train/val): 92.85/82.35, Loss(train/val) 0.20/0.76. Took 93.05 sec\n",
      "Epoch 146, Acc(train/val): 93.01/82.90, Loss(train/val) 0.20/0.71. Took 93.41 sec\n",
      "Epoch 147, Acc(train/val): 93.15/82.18, Loss(train/val) 0.19/0.78. Took 93.02 sec\n",
      "Epoch 148, Acc(train/val): 93.26/82.58, Loss(train/val) 0.19/0.72. Took 93.64 sec\n",
      "Epoch 149, Acc(train/val): 93.60/83.17, Loss(train/val) 0.18/0.71. Took 93.70 sec\n",
      "Epoch 150, Acc(train/val): 93.57/83.77, Loss(train/val) 0.18/0.69. Took 92.65 sec\n",
      "Epoch 151, Acc(train/val): 93.80/82.68, Loss(train/val) 0.17/0.77. Took 92.71 sec\n",
      "Epoch 152, Acc(train/val): 93.60/83.55, Loss(train/val) 0.18/0.70. Took 92.48 sec\n",
      "Epoch 153, Acc(train/val): 93.65/82.83, Loss(train/val) 0.17/0.77. Took 92.50 sec\n",
      "Epoch 154, Acc(train/val): 93.06/82.62, Loss(train/val) 0.19/0.75. Took 92.51 sec\n",
      "Epoch 155, Acc(train/val): 93.71/81.79, Loss(train/val) 0.18/0.83. Took 93.01 sec\n",
      "Epoch 156, Acc(train/val): 93.74/83.49, Loss(train/val) 0.17/0.68. Took 92.70 sec\n",
      "Epoch 157, Acc(train/val): 93.56/81.88, Loss(train/val) 0.18/0.84. Took 92.57 sec\n",
      "Epoch 158, Acc(train/val): 93.39/84.17, Loss(train/val) 0.18/0.68. Took 92.55 sec\n",
      "Epoch 159, Acc(train/val): 93.51/82.20, Loss(train/val) 0.18/0.84. Took 92.58 sec\n",
      "Epoch 160, Acc(train/val): 93.32/83.69, Loss(train/val) 0.18/0.69. Took 92.93 sec\n",
      "Epoch 161, Acc(train/val): 93.46/82.29, Loss(train/val) 0.19/0.83. Took 92.71 sec\n",
      "Epoch 162, Acc(train/val): 93.78/83.62, Loss(train/val) 0.17/0.71. Took 92.53 sec\n",
      "Epoch 163, Acc(train/val): 94.01/81.78, Loss(train/val) 0.17/0.89. Took 92.54 sec\n",
      "Epoch 164, Acc(train/val): 93.97/84.55, Loss(train/val) 0.17/0.70. Took 92.91 sec\n",
      "Epoch 165, Acc(train/val): 94.38/84.41, Loss(train/val) 0.15/0.69. Took 92.78 sec\n",
      "Epoch 166, Acc(train/val): 94.61/84.87, Loss(train/val) 0.15/0.68. Took 92.57 sec\n",
      "Epoch 167, Acc(train/val): 94.59/84.65, Loss(train/val) 0.15/0.69. Took 92.67 sec\n",
      "Epoch 168, Acc(train/val): 94.18/83.90, Loss(train/val) 0.16/0.72. Took 92.61 sec\n",
      "Epoch 169, Acc(train/val): 94.17/84.44, Loss(train/val) 0.16/0.71. Took 92.66 sec\n",
      "Epoch 170, Acc(train/val): 94.29/84.03, Loss(train/val) 0.16/0.73. Took 93.05 sec\n",
      "Epoch 171, Acc(train/val): 94.03/84.14, Loss(train/val) 0.16/0.71. Took 92.51 sec\n",
      "Epoch 172, Acc(train/val): 94.62/83.86, Loss(train/val) 0.15/0.75. Took 92.53 sec\n",
      "Epoch 173, Acc(train/val): 94.50/83.60, Loss(train/val) 0.15/0.78. Took 93.08 sec\n",
      "Epoch 174, Acc(train/val): 94.53/84.76, Loss(train/val) 0.15/0.69. Took 93.32 sec\n",
      "Epoch 175, Acc(train/val): 94.57/83.90, Loss(train/val) 0.15/0.77. Took 93.02 sec\n",
      "Epoch 176, Acc(train/val): 94.45/84.01, Loss(train/val) 0.15/0.71. Took 94.70 sec\n",
      "Epoch 177, Acc(train/val): 94.74/84.18, Loss(train/val) 0.15/0.73. Took 92.65 sec\n",
      "Epoch 178, Acc(train/val): 94.59/84.08, Loss(train/val) 0.15/0.74. Took 92.64 sec\n",
      "Epoch 179, Acc(train/val): 94.19/83.89, Loss(train/val) 0.16/0.76. Took 92.73 sec\n",
      "Epoch 180, Acc(train/val): 94.32/84.85, Loss(train/val) 0.15/0.66. Took 92.52 sec\n",
      "Epoch 181, Acc(train/val): 94.78/83.64, Loss(train/val) 0.14/0.77. Took 92.50 sec\n",
      "Epoch 182, Acc(train/val): 95.07/84.62, Loss(train/val) 0.14/0.68. Took 92.52 sec\n",
      "Epoch 183, Acc(train/val): 94.85/83.49, Loss(train/val) 0.14/0.80. Took 92.65 sec\n",
      "Epoch 184, Acc(train/val): 95.05/83.50, Loss(train/val) 0.14/0.75. Took 92.59 sec\n",
      "Epoch 185, Acc(train/val): 95.12/83.17, Loss(train/val) 0.13/0.81. Took 92.71 sec\n",
      "Epoch 186, Acc(train/val): 95.02/84.03, Loss(train/val) 0.14/0.76. Took 92.60 sec\n",
      "Epoch 187, Acc(train/val): 95.27/83.02, Loss(train/val) 0.13/0.80. Took 92.40 sec\n",
      "Epoch 188, Acc(train/val): 95.20/84.12, Loss(train/val) 0.13/0.72. Took 92.42 sec\n",
      "Epoch 189, Acc(train/val): 95.58/83.63, Loss(train/val) 0.13/0.77. Took 92.54 sec\n",
      "Epoch 190, Acc(train/val): 95.60/84.16, Loss(train/val) 0.12/0.73. Took 92.28 sec\n",
      "Epoch 191, Acc(train/val): 95.54/84.15, Loss(train/val) 0.13/0.77. Took 92.40 sec\n",
      "Epoch 192, Acc(train/val): 95.17/83.82, Loss(train/val) 0.13/0.78. Took 92.30 sec\n",
      "Epoch 193, Acc(train/val): 95.39/83.43, Loss(train/val) 0.13/0.80. Took 92.32 sec\n",
      "Epoch 194, Acc(train/val): 95.03/83.47, Loss(train/val) 0.13/0.83. Took 92.59 sec\n",
      "Epoch 195, Acc(train/val): 95.11/82.90, Loss(train/val) 0.14/0.88. Took 92.72 sec\n",
      "Epoch 196, Acc(train/val): 95.25/83.51, Loss(train/val) 0.13/0.85. Took 92.58 sec\n",
      "Epoch 197, Acc(train/val): 95.17/82.94, Loss(train/val) 0.13/0.89. Took 92.29 sec\n",
      "Epoch 198, Acc(train/val): 95.75/85.27, Loss(train/val) 0.12/0.71. Took 92.58 sec\n",
      "Epoch 199, Acc(train/val): 95.39/82.95, Loss(train/val) 0.13/0.87. Took 92.46 sec\n",
      "Epoch 200, Acc(train/val): 95.97/84.52, Loss(train/val) 0.11/0.77. Took 92.53 sec\n",
      "Epoch 201, Acc(train/val): 95.84/83.12, Loss(train/val) 0.11/0.87. Took 92.68 sec\n",
      "Epoch 202, Acc(train/val): 96.07/84.88, Loss(train/val) 0.11/0.75. Took 92.96 sec\n",
      "Epoch 203, Acc(train/val): 96.07/84.45, Loss(train/val) 0.11/0.78. Took 92.34 sec\n",
      "Epoch 204, Acc(train/val): 95.91/84.62, Loss(train/val) 0.11/0.78. Took 93.57 sec\n",
      "Epoch 205, Acc(train/val): 95.90/85.34, Loss(train/val) 0.11/0.71. Took 92.46 sec\n",
      "Epoch 206, Acc(train/val): 95.98/85.38, Loss(train/val) 0.11/0.73. Took 93.16 sec\n",
      "Epoch 207, Acc(train/val): 96.37/85.51, Loss(train/val) 0.10/0.73. Took 92.42 sec\n",
      "Epoch 208, Acc(train/val): 96.06/85.16, Loss(train/val) 0.11/0.78. Took 92.73 sec\n",
      "Epoch 209, Acc(train/val): 96.27/84.06, Loss(train/val) 0.10/0.81. Took 92.79 sec\n",
      "Epoch 210, Acc(train/val): 96.32/85.72, Loss(train/val) 0.10/0.72. Took 92.70 sec\n",
      "Epoch 211, Acc(train/val): 96.00/85.79, Loss(train/val) 0.11/0.74. Took 92.45 sec\n",
      "Epoch 212, Acc(train/val): 95.90/85.26, Loss(train/val) 0.11/0.76. Took 92.74 sec\n",
      "Epoch 213, Acc(train/val): 95.25/85.17, Loss(train/val) 0.13/0.76. Took 92.72 sec\n",
      "Epoch 214, Acc(train/val): 95.56/84.26, Loss(train/val) 0.12/0.79. Took 92.43 sec\n",
      "Epoch 215, Acc(train/val): 96.11/85.79, Loss(train/val) 0.11/0.74. Took 92.38 sec\n",
      "Epoch 216, Acc(train/val): 96.16/85.11, Loss(train/val) 0.11/0.75. Took 92.71 sec\n",
      "Epoch 217, Acc(train/val): 96.47/85.31, Loss(train/val) 0.10/0.77. Took 92.41 sec\n",
      "Epoch 218, Acc(train/val): 96.67/85.78, Loss(train/val) 0.09/0.70. Took 92.61 sec\n",
      "Epoch 219, Acc(train/val): 96.37/84.89, Loss(train/val) 0.10/0.84. Took 92.49 sec\n",
      "Epoch 220, Acc(train/val): 96.21/84.50, Loss(train/val) 0.10/0.78. Took 92.48 sec\n",
      "Epoch 221, Acc(train/val): 96.39/84.70, Loss(train/val) 0.10/0.80. Took 92.51 sec\n",
      "Epoch 222, Acc(train/val): 96.52/85.77, Loss(train/val) 0.10/0.73. Took 92.54 sec\n",
      "Epoch 223, Acc(train/val): 96.56/85.79, Loss(train/val) 0.10/0.74. Took 92.29 sec\n",
      "Epoch 224, Acc(train/val): 96.20/85.17, Loss(train/val) 0.10/0.74. Took 92.36 sec\n",
      "Epoch 225, Acc(train/val): 96.27/85.07, Loss(train/val) 0.10/0.79. Took 92.40 sec\n",
      "Epoch 226, Acc(train/val): 96.39/85.42, Loss(train/val) 0.10/0.74. Took 92.96 sec\n",
      "Epoch 227, Acc(train/val): 96.33/84.05, Loss(train/val) 0.10/0.83. Took 92.54 sec\n",
      "Epoch 228, Acc(train/val): 96.31/85.48, Loss(train/val) 0.10/0.73. Took 92.52 sec\n",
      "Epoch 229, Acc(train/val): 96.00/84.92, Loss(train/val) 0.11/0.76. Took 92.45 sec\n",
      "Epoch 230, Acc(train/val): 95.89/85.81, Loss(train/val) 0.11/0.74. Took 92.52 sec\n",
      "Epoch 231, Acc(train/val): 96.09/84.64, Loss(train/val) 0.11/0.80. Took 92.59 sec\n",
      "Epoch 232, Acc(train/val): 96.35/85.70, Loss(train/val) 0.10/0.74. Took 92.67 sec\n",
      "Epoch 233, Acc(train/val): 96.62/85.30, Loss(train/val) 0.09/0.77. Took 92.66 sec\n",
      "Epoch 234, Acc(train/val): 96.74/85.63, Loss(train/val) 0.09/0.74. Took 92.53 sec\n",
      "Epoch 235, Acc(train/val): 96.80/85.34, Loss(train/val) 0.09/0.74. Took 92.59 sec\n",
      "Epoch 236, Acc(train/val): 96.78/85.19, Loss(train/val) 0.09/0.76. Took 92.66 sec\n",
      "Epoch 237, Acc(train/val): 96.92/85.91, Loss(train/val) 0.08/0.74. Took 92.50 sec\n",
      "Epoch 238, Acc(train/val): 96.97/85.64, Loss(train/val) 0.08/0.73. Took 92.62 sec\n",
      "Epoch 239, Acc(train/val): 96.95/85.88, Loss(train/val) 0.08/0.75. Took 92.68 sec\n",
      "Epoch 240, Acc(train/val): 96.97/85.37, Loss(train/val) 0.08/0.76. Took 92.72 sec\n",
      "Epoch 241, Acc(train/val): 97.06/85.68, Loss(train/val) 0.08/0.77. Took 92.82 sec\n",
      "Epoch 242, Acc(train/val): 96.87/84.93, Loss(train/val) 0.09/0.80. Took 92.60 sec\n",
      "Epoch 243, Acc(train/val): 96.84/84.93, Loss(train/val) 0.09/0.80. Took 92.64 sec\n",
      "Epoch 244, Acc(train/val): 96.83/85.19, Loss(train/val) 0.09/0.78. Took 92.90 sec\n",
      "Epoch 245, Acc(train/val): 97.04/85.40, Loss(train/val) 0.08/0.80. Took 92.92 sec\n",
      "Epoch 246, Acc(train/val): 97.25/85.56, Loss(train/val) 0.08/0.79. Took 92.71 sec\n",
      "Epoch 247, Acc(train/val): 97.28/85.77, Loss(train/val) 0.07/0.77. Took 92.52 sec\n",
      "Epoch 248, Acc(train/val): 97.27/85.78, Loss(train/val) 0.08/0.74. Took 92.67 sec\n",
      "Epoch 249, Acc(train/val): 97.13/85.40, Loss(train/val) 0.08/0.78. Took 92.56 sec\n",
      "Epoch 250, Acc(train/val): 97.20/86.52, Loss(train/val) 0.08/0.73. Took 92.83 sec\n",
      "Epoch 251, Acc(train/val): 97.24/85.77, Loss(train/val) 0.08/0.77. Took 93.48 sec\n",
      "Epoch 252, Acc(train/val): 97.51/85.97, Loss(train/val) 0.07/0.76. Took 93.05 sec\n",
      "Epoch 253, Acc(train/val): 97.16/85.25, Loss(train/val) 0.08/0.79. Took 93.73 sec\n",
      "Epoch 254, Acc(train/val): 97.20/85.89, Loss(train/val) 0.08/0.77. Took 93.43 sec\n",
      "Epoch 255, Acc(train/val): 97.13/86.21, Loss(train/val) 0.08/0.75. Took 92.62 sec\n",
      "Epoch 256, Acc(train/val): 97.40/85.65, Loss(train/val) 0.07/0.79. Took 92.53 sec\n",
      "Epoch 257, Acc(train/val): 97.28/85.66, Loss(train/val) 0.08/0.81. Took 93.22 sec\n",
      "Epoch 258, Acc(train/val): 97.43/86.01, Loss(train/val) 0.07/0.75. Took 92.90 sec\n",
      "Epoch 259, Acc(train/val): 97.36/85.97, Loss(train/val) 0.08/0.75. Took 92.65 sec\n",
      "Epoch 260, Acc(train/val): 97.39/85.82, Loss(train/val) 0.07/0.78. Took 92.55 sec\n",
      "Epoch 261, Acc(train/val): 97.66/85.60, Loss(train/val) 0.07/0.79. Took 92.61 sec\n",
      "Epoch 262, Acc(train/val): 97.42/85.42, Loss(train/val) 0.07/0.78. Took 92.81 sec\n",
      "Epoch 263, Acc(train/val): 97.05/85.63, Loss(train/val) 0.08/0.76. Took 92.65 sec\n",
      "Epoch 264, Acc(train/val): 97.19/85.42, Loss(train/val) 0.08/0.81. Took 92.66 sec\n",
      "Epoch 265, Acc(train/val): 97.11/85.49, Loss(train/val) 0.08/0.79. Took 92.34 sec\n",
      "Epoch 266, Acc(train/val): 97.24/85.47, Loss(train/val) 0.08/0.81. Took 92.68 sec\n",
      "Epoch 267, Acc(train/val): 97.41/85.55, Loss(train/val) 0.07/0.76. Took 92.37 sec\n",
      "Epoch 268, Acc(train/val): 97.45/85.62, Loss(train/val) 0.07/0.80. Took 92.66 sec\n",
      "Epoch 269, Acc(train/val): 97.55/86.11, Loss(train/val) 0.07/0.77. Took 92.98 sec\n",
      "Epoch 270, Acc(train/val): 97.45/85.83, Loss(train/val) 0.07/0.77. Took 92.51 sec\n",
      "Epoch 271, Acc(train/val): 97.72/85.61, Loss(train/val) 0.06/0.80. Took 92.61 sec\n",
      "Epoch 272, Acc(train/val): 97.64/85.47, Loss(train/val) 0.07/0.78. Took 92.65 sec\n",
      "Epoch 273, Acc(train/val): 97.70/84.90, Loss(train/val) 0.06/0.85. Took 92.86 sec\n",
      "Epoch 274, Acc(train/val): 97.74/85.20, Loss(train/val) 0.06/0.87. Took 92.76 sec\n",
      "Epoch 275, Acc(train/val): 97.67/85.37, Loss(train/val) 0.07/0.82. Took 92.79 sec\n",
      "Epoch 276, Acc(train/val): 97.52/85.81, Loss(train/val) 0.07/0.80. Took 92.57 sec\n",
      "Epoch 277, Acc(train/val): 97.56/85.65, Loss(train/val) 0.07/0.83. Took 92.64 sec\n",
      "Epoch 278, Acc(train/val): 97.67/85.51, Loss(train/val) 0.06/0.83. Took 92.38 sec\n",
      "Epoch 279, Acc(train/val): 97.75/85.64, Loss(train/val) 0.06/0.82. Took 92.51 sec\n",
      "Epoch 280, Acc(train/val): 97.91/85.62, Loss(train/val) 0.06/0.80. Took 92.44 sec\n",
      "Epoch 281, Acc(train/val): 97.81/85.91, Loss(train/val) 0.06/0.82. Took 92.49 sec\n",
      "Epoch 282, Acc(train/val): 97.97/86.25, Loss(train/val) 0.06/0.77. Took 92.28 sec\n",
      "Epoch 283, Acc(train/val): 97.90/86.01, Loss(train/val) 0.06/0.83. Took 92.40 sec\n",
      "Epoch 284, Acc(train/val): 97.60/85.92, Loss(train/val) 0.07/0.79. Took 92.73 sec\n",
      "Epoch 285, Acc(train/val): 97.70/84.88, Loss(train/val) 0.06/0.87. Took 92.72 sec\n",
      "Epoch 286, Acc(train/val): 97.74/85.73, Loss(train/val) 0.06/0.79. Took 92.38 sec\n",
      "Epoch 287, Acc(train/val): 97.71/85.84, Loss(train/val) 0.07/0.81. Took 92.34 sec\n",
      "Epoch 288, Acc(train/val): 97.82/86.14, Loss(train/val) 0.06/0.80. Took 92.38 sec\n",
      "Epoch 289, Acc(train/val): 97.89/85.91, Loss(train/val) 0.06/0.80. Took 93.09 sec\n",
      "Epoch 290, Acc(train/val): 97.91/85.41, Loss(train/val) 0.06/0.83. Took 92.60 sec\n",
      "Epoch 291, Acc(train/val): 97.92/85.67, Loss(train/val) 0.06/0.85. Took 92.52 sec\n",
      "Epoch 292, Acc(train/val): 97.85/85.38, Loss(train/val) 0.06/0.83. Took 92.54 sec\n",
      "Epoch 293, Acc(train/val): 97.97/86.04, Loss(train/val) 0.06/0.81. Took 92.60 sec\n",
      "Epoch 294, Acc(train/val): 97.80/85.53, Loss(train/val) 0.06/0.83. Took 92.69 sec\n",
      "Epoch 295, Acc(train/val): 97.85/85.98, Loss(train/val) 0.06/0.82. Took 92.68 sec\n",
      "Epoch 296, Acc(train/val): 97.81/85.59, Loss(train/val) 0.06/0.82. Took 92.41 sec\n",
      "Epoch 297, Acc(train/val): 97.80/85.76, Loss(train/val) 0.07/0.83. Took 92.60 sec\n",
      "Epoch 298, Acc(train/val): 97.84/84.86, Loss(train/val) 0.06/0.87. Took 92.46 sec\n",
      "Epoch 299, Acc(train/val): 98.12/86.06, Loss(train/val) 0.05/0.85. Took 92.68 sec\n",
      "Accuracy of plane : 94 %\n",
      "Accuracy of   car : 98 %\n",
      "Accuracy of  bird : 83 %\n",
      "Accuracy of   cat : 65 %\n",
      "Accuracy of  deer : 80 %\n",
      "Accuracy of   dog : 88 %\n",
      "Accuracy of  frog : 94 %\n",
      "Accuracy of horse : 96 %\n",
      "Accuracy of  ship : 96 %\n",
      "Accuracy of truck : 85 %\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 275.375x216 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 275.375x216 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, drop_rate=0.0):\n",
    "        # out_planes => growth_rate를 입력으로 받게 된다.\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "        inter_planes = out_planes * 4  # bottleneck layer의 conv 1x1 filter chennel 수는 4*growth_rate이다.\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, inter_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(inter_planes)\n",
    "        self.conv2 = nn.Conv2d(inter_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.dropout = nn.Dropout(p=drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(self.relu(self.bn1(x)))\n",
    "        if self.drop_rate > 0:\n",
    "            out = self.dropout (out)\n",
    "        out = self.conv2(self.relu(self.bn2(out)))\n",
    "        if self.drop_rate > 0:\n",
    "            out = self.dropout (out)\n",
    "        return torch.cat([x, out], 1)  # 입력으로 받은 x와 새로 만든 output을 합쳐서 내보낸다\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, nb_layers, in_planes, growth_rate, block, drop_rate=0.0):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, growth_rate, nb_layers, drop_rate)\n",
    "\n",
    "    def _make_layer(self, block, in_planes, growth_rate, nb_layers, drop_rate):\n",
    "        layers = []\n",
    "        for i in range(nb_layers):\n",
    "            layers.append(block(in_planes + i * growth_rate, growth_rate, drop_rate))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "\n",
    "class TransitionBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, drop_rate=0.0):\n",
    "        super(TransitionBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.dropout = nn.Dropout(p=drop_rate)\n",
    "        self.avg_pooling = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(self.relu(self.bn1(x)))\n",
    "        if self.drop_rate > 0:\n",
    "            out = self.dropout(out)\n",
    "        return self.avg_pooling(out)\n",
    "\n",
    "\n",
    "class Densenet(nn.Module):\n",
    "    def __init__(self, layers=121, num_classes=10, growth_rate=12, compression_factor=0.5, drop_rate=0.2):\n",
    "        super(Densenet, self).__init__()\n",
    "        num_of_blocks = 3\n",
    "        n = (layers - num_of_blocks - 1) / num_of_blocks  # 총 layers에서 첫 conv , 2개의 transit , 마지막 linear 빼고 / num_of_blocks\n",
    "        in_planes = 2 * growth_rate  # 논문에서 Bottleneck + Compression 할 경우 first layer은 2*growth_rate라고 했다.\n",
    "        n = n / 2  # conv 1x1 레이어가 추가되니까 !\n",
    "        block = BottleneckBlock\n",
    "\n",
    "\n",
    "        n = int(n)  # n = DenseBlock에서 block layer 개수를 의미한다.\n",
    "        self.conv1 = nn.Conv2d(3, in_planes, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)  # input:RGB -> output:growthR*2\n",
    "\n",
    "        # 1st block\n",
    "        # nb_layers,in_planes,growth_rate,block,drop_rate\n",
    "        self.block1 = DenseBlock(n, in_planes, growth_rate, block, drop_rate)\n",
    "        in_planes = int(in_planes + n * growth_rate)  # 입력 + 레이어 만큼의 growth_rate\n",
    "\n",
    "        # in_planes,out_planes,drop_rate\n",
    "        self.trans1 = TransitionBlock(in_planes, int(math.floor(in_planes * compression_factor)), drop_rate=drop_rate)\n",
    "        in_planes = int(math.floor(in_planes * compression_factor))\n",
    "\n",
    "        # 2nd block\n",
    "        # nb_layers,in_planes,growth_rate,block,drop_rate\n",
    "        self.block2 = DenseBlock(n, in_planes, growth_rate, block, drop_rate)\n",
    "        in_planes = int(in_planes + n * growth_rate)  # 입력 + 레이어 만큼의 growth_rate\n",
    "\n",
    "        # in_planes,out_planes,drop_rate\n",
    "        self.trans2 = TransitionBlock(in_planes, int(math.floor(in_planes * compression_factor)), drop_rate=drop_rate)\n",
    "        in_planes = int(math.floor(in_planes * compression_factor))\n",
    "\n",
    "        # 3rd block\n",
    "        # nb_layers,in_planes,growth_rate,block,drop_rate\n",
    "        self.block3 = DenseBlock(n, in_planes, growth_rate, block, drop_rate)\n",
    "        in_planes = int(in_planes + n * growth_rate)  # 입력 + 레이어 만큼의 growth_rate\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.ada_avg_pooling = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.fc = nn.Linear(in_planes, num_classes)  # 마지막에 ave_pool 후에 1x1 size의 결과만 남음.\n",
    "\n",
    "        self.in_planes = in_planes\n",
    "\n",
    "        # module 초기화\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # Conv layer들은 필터에서 나오는 분산 root(2/n)로 normalize 함\n",
    "                # mean = 0 , 분산 = sqrt(2/n) // 이게 무슨 초기화 방법이었는지 기억이 안난다.\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):  # shifting param이랑 scaling param 초기화(?)\n",
    "                m.weight.data.fill_(1)  #\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):  # linear layer 초기화.\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : 32*32\n",
    "        out = self.conv1(x)  # 32*32\n",
    "        out = self.block1(out)  # 32*32\n",
    "        out = self.trans1(out)  # 16*16\n",
    "        out = self.block2(out)  # 16*16\n",
    "        out = self.trans2(out)  # 8*8\n",
    "        out = self.block3(out)  # 8*8\n",
    "        out = self.relu(self.bn1(out))  # 8*8\n",
    "        out = self.ada_avg_pooling(out)  # 1*1\n",
    "        out = out.view(-1, self.in_planes)  # channel수만 남기 때문에 Linear -> in_planes\n",
    "        return self.fc(out)\n",
    "\n",
    "# layers,num_classes <- cifar '10' ,growth_rate=12,compression_factor=0.5,bottleneck=True,drop_rate=0.0\n",
    "\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = \"exp1_lr\"\n",
    "#models = ['CNN', 'Resnet', 'Densenet']\n",
    "args.model = 'Densenet'\n",
    "args.act = 'relu'\n",
    "args.l2 = 0.00001\n",
    "args.optim = 'SGD'  # 'RMSprop' #SGD, RMSprop, ADAM...\n",
    "args.lr = 1e-3\n",
    "args.epoch = 300\n",
    "\n",
    "args.train_batch_size = 128\n",
    "args.test_batch_size = 64\n",
    "\n",
    "\n",
    "\n",
    "#torchsummary.summary(Densenet().cuda(), (3, 32, 32))\n",
    "print(args)\n",
    "setting, result = experiment(partition, deepcopy(args))\n",
    "plot_loss_variation(result)\n",
    "plot_acc_variation(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: More case studies\n",
    "### 3.4.7 Sqeeze-and-excitation networks\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide87.PNG?raw=true)\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/SENet_Fig_3.png?raw=true)\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/excitation.png?raw=true)\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/activations_excitation.png?raw=true)\n",
    "\n",
    "    최근의 연구에 따르면, 학습 메커니즘을 feature 간의 spatial correlation을 캡처하는데 도움이 되는 네트워크에 통합함으로써, CNN에 의해 생성된 representation을 강화할 수 있다.\n",
    "    이 논문에서는 다른 측면으로, channel 간의 relationship을 조사한다.\n",
    "    본문에서는 convolutional feature의 channel 간의 상호 의존성을 명시적으로 모델링하는 Squeeze-and-Excitation(SE) block을 소개\n",
    "    네트워크가 feature recalibration을 수행할 수 있도록 해주는 메커니즘을 제안한다. 이 메커니즘을 통하면, global information으로부터 유익한 feature를 선택적으로 강조하면서 덜 유용한 feature를 억제하는 방법을 배울 수 있다.\n",
    "    Squeeze: Global Information Embedding\n",
    "    Excitation: Adaptive Recalibration (self-attetion on chennel)\n",
    "    이를 가지고 기존 네트워크를 거친 피쳐맵들을 feature recalibration(재조정)할 수 있다고 합니다. 자세히 살펴보면, SE는각 피쳐맵에 대한 전체 정보를 요약하는 Squeeze operation, 이를 통해 각 피쳐맵의 중요도를 스케일해주는 excitation operation으로 이루어져 있습니다.\n",
    "    파라미터의 증가량에 비해 모델 성능 향상도가 매우 큽니다. 이는 모델 복잡도(Model complexity)와 계산 복잡도(computational burden)이 크게 증가하지 않다는 장점이 있습니다.\n",
    "    또한, SE block은 residual network에도 직접 사용될 수 있다. (Fig.3 참조)\n",
    "    original ResNet-50에 비해, 0.26%의 상대적 증가에 해당한다.(GFLOPs)\n",
    "    최근의 연구에 따르면, 학습 메커니즘을 feature 간의 spatial correlation을 캡처하는데 도움이 되는 네트워크에 통합함으로써, CNN에 의해 생성된 representation을 강화할 수 있다.\n",
    "    여기서, SE block의 transformation F_tr은 residual module의 non-identity branch로 사용된다. 즉, Squeeze와 Excitation은 모두 identity branch와 summation 되기 전에 수행된다.\n",
    "\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(act='relu', epoch=300, exp_name='exp1_lr', l2=1e-05, lr=0.001, model='SE_Resnet', optim='SGD', test_batch_size=64, train_batch_size=256)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 8.00 GiB total capacity; 6.20 GiB already allocated; 23.75 MiB free; 139.80 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-3-52c0ffd23942>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[1;31m#torchsummary.summary(Resnet().cuda(), (3, 32, 32))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 113\u001B[1;33m \u001B[0msetting\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mexperiment\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpartition\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdeepcopy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    114\u001B[0m \u001B[0mplot_loss_variation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[0mplot_acc_variation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-1-ab8b5c6380c8>\u001B[0m in \u001B[0;36mexperiment\u001B[1;34m(partition, args)\u001B[0m\n\u001B[0;32m    218\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# loop over the dataset multiple times\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    219\u001B[0m         \u001B[0mts\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 220\u001B[1;33m         \u001B[0mnet\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_acc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpartition\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    221\u001B[0m         \u001B[0mval_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_acc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvalidate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpartition\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    222\u001B[0m         \u001B[0mte\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-1-ab8b5c6380c8>\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(net, partition, optimizer, criterion, args)\u001B[0m\n\u001B[0;32m    120\u001B[0m         \u001B[0minputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    121\u001B[0m         \u001B[0mlabels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 122\u001B[1;33m         \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnet\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    123\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    124\u001B[0m         \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    539\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    540\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 541\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    542\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    543\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-3-52c0ffd23942>\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     80\u001B[0m         \u001B[1;31m#x = self.maxpool(x)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     81\u001B[0m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayer1\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 82\u001B[1;33m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayer2\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     83\u001B[0m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayer3\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     84\u001B[0m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayer4\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    539\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    540\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 541\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    542\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    543\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m     90\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     91\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_modules\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 92\u001B[1;33m             \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     93\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     94\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    539\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    540\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 541\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    542\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    543\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-3-52c0ffd23942>\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     43\u001B[0m         \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbn2\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconv2\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m         \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbn3\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconv3\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 45\u001B[1;33m         \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     46\u001B[0m         \u001B[0mout\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshortcut\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresidual\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     47\u001B[0m         \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    539\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    540\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 541\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    542\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    543\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-3-52c0ffd23942>\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     14\u001B[0m         \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mavg_pool\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mc\u001B[0m\u001B[1;33m)\u001B[0m     \u001B[1;31m# [b,c,1,1] -> [b,c]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m         \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m     \u001B[1;31m# [b,c] -> [b,c,1,1]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 16\u001B[1;33m         \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mx\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexpand_as\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m              \u001B[1;31m# [b,c,1,1] -> [b,c,width,height]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     17\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 8.00 GiB total capacity; 6.20 GiB already allocated; 23.75 MiB free; 139.80 MiB cached)"
     ]
    }
   ],
   "source": [
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction):\n",
    "        super(SELayer, self).__init__()\n",
    "        # squeeze\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        # excitation\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)     # [b,c,1,1] -> [b,c]\n",
    "        y = self.fc(y).view(b, c, 1, 1)     # [b,c] -> [b,c,1,1]\n",
    "        y = x * y.expand_as(x)              # [b,c,1,1] -> [b,c,width,height]\n",
    "        return y\n",
    "\n",
    "class SE_Bottleneck(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1, reduction=16):\n",
    "        super(SE_Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, 4*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(4*planes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != 4 * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, 4 * planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(4 * planes)\n",
    "            )\n",
    "        self.se = SELayer(planes * 4, reduction)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out = self.se(out)\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# resnet-50\n",
    "class SE_Resnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SE_Resnet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        # self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)    # for cifar-10\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)   # for cifar-10\n",
    "        self.layer1 = self.residual_block(64, 3, stride=1)\n",
    "        self.layer2 = self.residual_block(128, 4, stride=2)\n",
    "        self.layer3 = self.residual_block(256, 6, stride=2)\n",
    "        self.layer4 = self.residual_block(512, 3, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(2048, 10)\n",
    "\n",
    "\n",
    "    def residual_block(self, planes, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(SE_Bottleneck(self.inplanes , planes, stride))\n",
    "        self.inplanes = planes * 4\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(SE_Bottleneck(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = \"exp1_lr\"\n",
    "#models = ['CNN', 'Resnet', 'SE_Resnet']\n",
    "args.model = 'SE_Resnet'\n",
    "args.act = 'relu'\n",
    "args.l2 = 0.00001\n",
    "args.optim = 'SGD'  # 'RMSprop' #SGD, RMSprop, ADAM...\n",
    "args.lr = 1e-3\n",
    "args.epoch = 300\n",
    "\n",
    "args.train_batch_size = 256\n",
    "args.test_batch_size = 64\n",
    "\n",
    "\n",
    "#torchsummary.summary(Resnet().cuda(), (3, 32, 32))\n",
    "print(args)\n",
    "setting, result = experiment(partition, deepcopy(args))\n",
    "plot_loss_variation(result)\n",
    "plot_acc_variation(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Going deeper: More Case studies\n",
    "### 3.4.7 State-of-the-art\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide89.PNG?raw=true)\n",
    "\n",
    "    g\n",
    "\n",
    "\n",
    "\n",
    "***\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}