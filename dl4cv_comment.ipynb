{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning for Computer Vision\n",
    "***\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/498_FA2019_lecture01.pdf_page_10.png?raw=true)\n",
    "## Plan for this lecture\n",
    "1. **Deep Learning** ~~for Computer Vision~~\n",
    "2. **Deep Learning for Computer Vision**\n",
    "\n",
    "\n",
    "    이번 영상에서는 CNN에 대해서 다뤄볼건데요, 컴퓨터 비전을 위한 딥러닝에는 여러 기술들이 있지만\n",
    "    현재 대표적인 컴퓨터비전 딥러닝 기술이라고 하면 CNN을 빼놓을 수 없기에 CNN에 집중해서 배워볼 생각입니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print('ready.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. **Deep Learning** ~~for Computer Vision~~\n",
    "### 1.1  Recap:Deep Learning\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/498_FA2019_lecture01.pdf_page_09.jpg?raw=true)\n",
    "\n",
    "    CNN을 배우기 전에 먼저 딥러닝의 중요한 부분들을 되짚어보겠습니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1.1  Real neuron\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide2.PNG?raw=true)\n",
    "\n",
    "    이 그림은 모든 Neural Network을 얘기할때 꼭 나오는 뉴런 그림이죠? 지겨운 분들도 있을 것 같습니다.\n",
    "    생물학적 백그라운드를 조금 알고 가면 좋을 것 같아 다시 준비 했습니다.\n",
    "    키워드는,\n",
    "    수상돌기(Dendrite)에서 다른 뉴런들의 신호를 받아 soma쪽에서 상태값을 이용하여 매우 간단한 연산을하고 축색돌기(Axon)을 통해\n",
    "    다른 뉴런들에게 전달할지 말지 결정합니다. NN은 이런 인간의 뇌 구조를 모방하여 만들었습니다.\n",
    "    (인간의 뇌에는 860억개의 뉴런이 있다고합니다.)\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1.1  Real neuron\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide90.PNG?raw=true)\n",
    "\n",
    "    이 뉴런을 단순화해서 표현을 할껀데, 요게 뭡니까? 소죠?\n",
    "    이게 피카소의 작품인데요, 소에서 점점 단순화되다가 결국 이렇게 되버렸죠.\n",
    "    뿔은 무슨 더듬이처럼 됬고, 꼬리가 있네요, 다리도 네개고.\n",
    "    그리고 몸통에.. 수컷이네요 수컷.\n",
    "\n",
    "\n",
    "    자 그러면 뉴런은 어떻게 표현할까냐는건데, 엄청 간단합니다.\n",
    "    소에서 저렇게 특징적인것만 표현했듯이\n",
    "    뉴런도 다른 뉴런들에게서 받아 출력을 내는 그림으로 간단히 표현할 수 있습니다.\n",
    "    이게 여러개 있으면 인공신경망, 뉴럴네트워크다 이렇게 말할 수 있습니다.\n",
    "\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1.2  Artificial neuron\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide3.PNG?raw=true)\n",
    "\n",
    "    요 뉴런을 computer science 측면에서 조금 더 자세히 보자면.\n",
    "    인공신경망의 뉴런 구조는 상태를 저장할 수 있는 weight ,bias 값을 이용하여\n",
    "    input을 다음 뉴런에게 전달할지 결정하는 구조로 만들어져 있습니다.\n",
    "    오해하지말아야 할 것은 뇌구조와 같게 만들었다는 뜻은 아니며,\n",
    "    신경생리학적 관찰적인 부분을 모델링한 것으로 이해해주심 되겠습니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1.2  Artificial neuron\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide4.PNG?raw=true)\n",
    "\n",
    "    위 그래프를 보면서 설명드리자면, 파란,붉은점들은 input이라 볼 수 있으며,\n",
    "    점선으로 표시된 부분의 뉴런의 weight를 표현하고 있습니다.\n",
    "    다양한 점선들은 weight에 따라 input을 어떻게 프로젝션 할 수 있는 지 보여주고 있습니다.\n",
    "\n",
    "    그럼 이제 파란점과 붉은점을 나누는 간단한 인공신경망을 만들어 보겠습니다\n",
    "***\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "torch.manual_seed(777)\n",
    "\n",
    "# input\n",
    "X = torch.FloatTensor([[0, 0], [1, 1]])\n",
    "Y = torch.FloatTensor([[0],[1]])\n",
    "\n",
    "# nn layers\n",
    "input_layer = torch.nn.Linear(2, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "# model\n",
    "model = torch.nn.Sequential(input_layer, sigmoid)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)  # modified learning rate from 0.1 to 1\n",
    "\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "\n",
    "    # cost/loss function\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(step, cost.item())\n",
    "\n",
    "# for drawing\n",
    "model_params = list(model.parameters())\n",
    "model_weights = model_params[0].data.numpy()\n",
    "model_bias = model_params[1].data.numpy()\n",
    "\n",
    "plt.scatter(X.numpy()[[0,-1], 0], X.numpy()[[0, -1], 1], s=50)\n",
    "plt.scatter(X.numpy()[[1,-1], 0], X.numpy()[[1, -1], 1], c='red', s=50)\n",
    "\n",
    "x_1 = np.arange(-0.1, 1.1, 0.1)\n",
    "y_1 = ((x_1 * model_weights[0,0]) + model_bias[0]) / (-model_weights[0,1])\n",
    "plt.plot(x_1, y_1)\n",
    "\n",
    "plt.legend([\"neuron_1\"], loc=8)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1.3 Limitations\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide6.PNG?raw=true)\n",
    "\n",
    "    이러한 방법론으로 인공신경망이 환대를 받고 있을때, xor problem이라는 심각한 문제가 등장합니다.\n",
    "    선을 아무리 잘 그어도 위와같은 문제는 해결할 수 없었습니다.\n",
    "\n",
    "\n",
    "    코드를 살펴보자면,\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.manual_seed(777)\n",
    "\n",
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]])\n",
    "\n",
    "input_layer = torch.nn.Linear(2, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "model = torch.nn.Sequential(input_layer, sigmoid)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)  # modified learning rate from 0.1 to 1\n",
    "\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(step, cost.item())\n",
    "\n",
    "model_params = list(model.parameters())\n",
    "model_weights = model_params[0].data.numpy()\n",
    "model_bias = model_params[1].data.numpy()\n",
    "\n",
    "plt.scatter(X.numpy()[[0,-1], 0], X.numpy()[[0, -1], 1], s=50)\n",
    "plt.scatter(X.numpy()[[1,2], 0], X.numpy()[[1, 2], 1], c='red', s=50)\n",
    "\n",
    "x_1 = np.arange(-0.1, 1.1, 0.1)\n",
    "y_1 = ((x_1 * model_weights[0,0]) + model_bias[0]) / (-model_weights[0,1])\n",
    "plt.plot(x_1, y_1)\n",
    "\n",
    "plt.legend([\"neuron_1\"], loc=8)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1.4  First AI Winter\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/ai_winter.png?raw=true)\n",
    "\n",
    "    이러한 간단한 문제도 풀지 못한다면서 손가락 질과 함께 첫번째 AI 암흑기로 돌입하게 됩니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Multi-layer Perceptron\n",
    "#### 1.2.1 XOR Problem\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/xor.png?raw=true)\n",
    "\n",
    "    XOR은 a,b가 서로 다를때 1이고 같을 때 0을 표현하는 함수를 뜻합니다.\n",
    "    보니까 직선 하나로는 분류를 할 수 없겠다고 생각해서, 직선이 더 있으면 될 것 같다는 생각을 하게 됩니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2.2 Solving XOR Problem\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide11.PNG?raw=true)\n",
    "\n",
    "    전에는 x1, x2 에서 바로 y로 전달해었는데, 이제는 중간에 뭐가 하나 생겼죠?\n",
    "    이것을 히든레이어(은닉층)이라고 하고 Multi-layer Perceptron이 등장하게 됩니다.\n",
    "    수식이 좀 어지러워 보이는데 하나만 따라가보죠.\n",
    "    우선 앞에가 a, 뒤가 b입니다. 결과만 보자면 같을때는 0, 다를때는 1값이 잘 나오고 있네요.\n",
    "    (맨 아랫줄 하나씩 설명)\n",
    "    그럼 코드로 구현해볼까요?\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "torch.manual_seed(777)\n",
    "\n",
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]])\n",
    "\n",
    "# nn layers\n",
    "input_layer = torch.nn.Linear(2, 2, bias=True)\n",
    "hidden = torch.nn.Linear(2, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "# model\n",
    "model = torch.nn.Sequential(input_layer, sigmoid, hidden, sigmoid)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)  # modified learning rate from 0.1 to 1\n",
    "\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "\n",
    "    # cost/loss function\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(step, cost.item())\n",
    "\n",
    "model_params = list(model.parameters())\n",
    "model_weights = model_params[0].data.numpy()\n",
    "model_bias = model_params[1].data.numpy()\n",
    "\n",
    "plt.scatter(X.numpy()[[0,-1], 0], X.numpy()[[0, -1], 1], s=50)\n",
    "plt.scatter(X.numpy()[[1,2], 0], X.numpy()[[1, 2], 1], c='red', s=50)\n",
    "\n",
    "x_1 = np.arange(-0.1, 1.1, 0.1)\n",
    "y_1 = ((x_1 * model_weights[0,0]) + model_bias[0]) / (-model_weights[0,1])\n",
    "plt.plot(x_1, y_1)\n",
    "\n",
    "x_2 = np.arange(-0.1, 1.1, 0.1)\n",
    "y_2 = ((x_2 * model_weights[1,0]) + model_bias[1]) / (-model_weights[1,1])\n",
    "plt.plot(x_2, y_2)\n",
    "\n",
    "plt.legend([\"neuron_1\", \"neuron_2\"], loc=8)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    하나의 히든레이어를 추가함으로써 XOR문제를 해결하는 것을 볼 수 있습니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2.3  Multi-layer Perceptron\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide13.PNG?raw=true)\n",
    "\n",
    "    우리는 이러한 문제에 좀 더 직관적으로 이해할 필요가 있습니다.\n",
    "    이해를 돕기위해 오른쪽 붉은점과 파란점을 색상을 좀 바꾸겠습니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2.3  Multi-layer Perceptron\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide14.PNG?raw=true)\n",
    "\n",
    "    뉴런이 어떠한 방향으로 프로젝션을 할 수 있다고 생각해보시면,\n",
    "    프로젝션은 그위치에 어느 한 방향으로 본다고 생각하시면 편합니다.\n",
    "    오른쪽위에서 왼쪽아래로 프로젝션하는 뉴런에게 보이는 모습은 오른쪽과 같을 겁니다.\n",
    "    보라색은 왼쪽에 있고, 초록과 빨강은 겹쳐보일테고, 파랑은 오른쪽에 있겠죠\n",
    "    다른 뉴런은 아래와 왼쪽과 같이 보일겁니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2.3  Multi-layer Perceptron\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide15.PNG?raw=true)\n",
    "\n",
    "    sigmoid함수를 지나면 0~1 사이에 값들로 치환이 될겁니다.\n",
    "    0과 0이하의 값들은 0에 가깝게 모일테고 4를 넘는값들은 1에 가깝게 모이겠죠?\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2.3  Multi-layer Perceptron\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide16.PNG?raw=true)\n",
    "\n",
    "    그럼 output 뉴런에 관점에서 오른쪽아래에서 왼쪽위로 다시 프로젝트를 해보면\n",
    "    초록과빨강은 모이게되고, 파랑과 보라가 모이게 될 겁니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2.3  Multi-layer Perceptron\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide17.PNG?raw=true)\n",
    "[Multi-layer Perceptron](https://playground.tensorflow.org)\n",
    "\n",
    "    0.5의 값으로 나누게 되면, 빨강과 파랑을 분류할 수 있습니다.\n",
    "    이렇게 Neural Network의 layer들을 한층 씩 통과하면서 고차원적인 차원으로 이동하여,\n",
    "    xor문제를 해결할 수 있게 된다는 것을 직관적으로 이해할 수 있습니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3  Stochastic Gradient Decent\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/sgd.png?raw=true)\n",
    "\n",
    "    여기서 어떻게 이 많은 뉴런들의 파라미터 값들을 학습할 것인지 문제가 생깁니다.\n",
    "    여기서 Gradient Decent(경사하강법)이라는 방법론이 등장합니다.\n",
    "    개념적으로 되짚어 보자면,\n",
    "    최종적으로 나온 loss를 역전파를 진행하면서 각각의 뉴런의 weight에 대해서 편미분을 하면\n",
    "    gradient가 나오는데, gradient가 낮아지는 방향으로 스텝스텝이동하면 전체 loss가 줄게됩니다.\n",
    "    앞에 Stochastic이 붙은 이유는 전체 데이터를 한번에 보지 않고,\n",
    "    mini-batch, 즉 데이터를 몇개 단위로 쪼개서 학습을 진행하면서 확률적으로 이동하기에 이런 이름이 붙었습니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 Vanishing Gradient\n",
    "#### 1.4.1  Second AI Winter\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/ai_winter.png?raw=true)\n",
    "\n",
    "    그럼에도 불구하고 두번째 AI 암흑기가 찾아오게 됩니다.\n",
    "    그 원인은 Vanishing Gradient(기울기소실) & Overfitting(과적합) 문제입니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4.2 Vanishing Gradient Problem\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide22.PNG?raw=true)\n",
    "\n",
    "    Vanishing Gradient는 뉴럴 네트워크를 깊게 쌓다보니,\n",
    "    역전파가 잘 되지 않아 앞에 레이어들이 업데이트가 안되는 상황입니다.\n",
    "    문제는 sigmoid activation function을 보면 알 수 있습니다.\n",
    "    역전파를 할때 sigmoid의 도함수는 최대 0.25이기 때문에, 역전파를 하면 할 수록\n",
    "    값이 1/4씩 작아진다는 문제점이 있었습니다.\n",
    "    이 문제를 해결하기 위해 등장한 것이 아래보이는 RRRRReLU입니다.\n",
    "    Relu는 양수 일 때 y=x값이 되며,\n",
    "    특히 역전파를 할때 Relu의 도함수는 x는 0보다 크면 1 같거나 작으면 0이라는 도함수를 가지기 때문에\n",
    "    VG문제를 해결할 수 있었습니다.\n",
    "    여기서 \"아니 그러면, 0이하일때도 그냥 y=x형태로 만들어버리면 안되? 느낌 좋은데?\"라는 생각이 드실 수도 있을텐데\n",
    "    그렇게 되면 non-linear한 성질을 잃어버리기 때문에 함수표현에 제한이 생깁니다.\n",
    "    그래서 ReLU의 장점은 non-linear한 성질을 가지면서도 VG문제를 해결했다고 생각하시면 됩니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.4.2 Dropout\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/drop_out.png?raw=true)\n",
    "\n",
    "    또하나의 문제는 학습데이터를 외워버려서,\n",
    "    학습데이터에만 잘 동작하고 새로운 데이터에는 이상한 결과를 내놓는 overfitting 문제가 있었는데,\n",
    "    보시면 뉴런들을 학습도중 랜덤하게 끊어서 학습하여 variance를 줄임으로써\n",
    "    overfitting을 방지하는 방법으로 해결하였습니다.\n",
    "    variance가 커지면 과하게 함수를 표현할 수 있기 때문에, 이걸 좀 normalization 해준다고 생각하시면 됩니다.\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5 What is the DL?\n",
    "![pt](https://github.com/tiktakdad/dl4cv/blob/master/images/pt/Slide25.PNG?raw=true)\n",
    "\n",
    "    딥러닝의 대가증 한명이자 튜링 수상자인 얀 르쿤 교수님이 딥러닝에 대한 정의를\n",
    "    Deep Learning 마지막 장표로 채워넣으면서 이것으로 Deep Learning 핵심 되짚기는 끝내고\n",
    "    드디어 Deep Learning for Computer Vision에 대한 이야기를 시작할까 합니다.\n",
    "\n",
    "    \"딥러닝은 패러미터화된 모듈들을 구축하고, 기울기 기반의 최적화를 통해\n",
    "    데이터를 학습시키는 것이다. 그게 다다.\"\n",
    "\n",
    "    (결국 이 이야기를 하려고 했는데 먼길을 돌아온 것 같습니다. 그럼 잠시 질문을 받고, 10분정도 쉬는시간을 갖겠습니다.)\n",
    "***\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}